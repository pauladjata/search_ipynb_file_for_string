{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dates are uniform\n",
    "## takes the first set of dates in a df filtered by filename and director name, and then checks that the dates in row[1:End] are the same as in row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from pandas import ExcelWriter\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import cycle, zip_longest\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import stat\n",
    "\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GRAND_SOURCE = r'C:\\Users\\Paul\\backup_1Nov19\\adjata\\21_PICKLE_FILES\\df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_24JUL20_III.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle_file_as_df(path_and_fnme_string):\n",
    "    \n",
    "    pickle_in = open(path_and_fnme_string,\"rb\")\n",
    "    df_pickle = pickle.load(pickle_in)\n",
    "    \n",
    "    return df_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFileAsPickle(path_and_fnme_string, file_to_save):\n",
    "    \n",
    "    pickle_out = open(path_and_fnme_string,\"wb\")\n",
    "    pickle.dump(file_to_save, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAsExcelFile(filename, df):\n",
    "    \n",
    "    options = {}\n",
    "    options['strings_to_formulas'] = False\n",
    "    options['strings_to urls'] = False\n",
    "\n",
    "    file_name = filename.replace('.AX', \"\") + r'.xlsx'\n",
    "\n",
    "    # file_to_save = data_folder / file_name\n",
    "\n",
    "    file_to_save = file_name\n",
    "\n",
    "    writer = ExcelWriter(file_to_save, options=options)\n",
    "    df.to_excel(writer, 'title')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openExcelFile(file_name):\n",
    "\n",
    "    workbook = pd.ExcelFile(file_name)\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for sheet_name in workbook.sheet_names:\n",
    "        df = workbook.parse(sheet_name)\n",
    "        d[sheet_name] = df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle_file_and_save_as_excel_file(path_and_fnme_string_of_pickle_file, filename_to_save_as_excel_file):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    df_temp = open_pickle_file_as_df(path_and_fnme_string_of_pickle_file)\n",
    "    \n",
    "    filepath_filename_tuple = os.path.split(os.path.abspath(path_and_fnme_string_of_pickle_file))\n",
    "    \n",
    "    excel_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_to_save_as_excel_file.replace('.xlsx','')\n",
    "    \n",
    "    print(excel_filepath_and_filename)\n",
    "    \n",
    "    saveAsExcelFile(excel_filepath_and_filename, df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create mirror of pickle file in use and save as Excel file for modifications\n",
    "# open_pickle_file_and_save_as_excel_file(pickle_file_path_and_name_GRAND_SOURCE, 'test_Excel_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_excel_file_and_save_as_pickle_file(path_and_fnme_string_of_excel_file, filename_to_save_as_pickle_file):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    df_temp = openExcelFile(path_and_fnme_string_of_excel_file)\n",
    "    \n",
    "    print('opened Excel file')\n",
    "    \n",
    "    if 'Column1' in df_temp.columns:\n",
    "    \n",
    "        df_temp.drop(['Column1'], axis=1, inplace=True)\n",
    "\n",
    "    if 'Unnamed: 0' in df_temp.columns:\n",
    "\n",
    "        df_temp.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "        \n",
    "    filepath_filename_tuple = os.path.split(os.path.abspath(path_and_fnme_string_of_excel_file))\n",
    "    \n",
    "    pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_to_save_as_pickle_file.replace('.pickle','') + r'.pickle'\n",
    "    \n",
    "    print(pickle_filepath_and_filename)\n",
    "    \n",
    "    saveFileAsPickle(pickle_filepath_and_filename, df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open adjusted Excel file and save as pickle file\n",
    "# open_excel_file_and_save_as_pickle_file(r'C:\\Users\\Paul\\backup_1Nov19\\python_stock_APIs\\review_files\\processing_completed_sub_files\\split_files\\df_SINGLE_3Z_II.xlsx', r'df_SINGLE_3Z_II.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openPDFFileFromFileName(pdf_name):\n",
    "    \n",
    "    import subprocess\n",
    "    import os\n",
    "\n",
    "    doc_id = str(pdf_name)\n",
    "    doc_ext_pdf = str('pdf')\n",
    "    doc_ext_txt = str('txt')\n",
    "\n",
    "    doc_id = doc_id.replace('.pdf', '').replace('.txt', '')\n",
    "\n",
    "    # doc_path = str(r'C:\\Users\\paul_\\Documents\\adjata\\1_data_testing_Feb19')\n",
    "    doc_path = str(r'C:\\Users\\Paul\\backup_1Nov19\\adjata\\1_data_testing_Feb19')\n",
    "    doc_year = 2008\n",
    "\n",
    "    while doc_year < 2019:\n",
    "\n",
    "        path_to_file_to_open_pdf = doc_path + '\\\\' + 'COPIED_DOCUMENTS_' + str(doc_year) + '.' + doc_ext_txt\n",
    "        path_to_file_to_open_txt = doc_path + '\\\\' + 'COPIED_DOCUMENTS_TEXT_' + str(doc_year) + '.' + doc_ext_txt\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        with open(path_to_file_to_open_pdf, 'r') as fp:\n",
    "            lines = fp.read().splitlines()\n",
    "            for l in lines:\n",
    "                if doc_id in l and len(l[-(len(l) - l.rfind('\\\\')) + 1:].replace('.pdf', '')) == len(doc_id):\n",
    "                    cnt = cnt + 1\n",
    "                    full_path_to_doc_to_open_pdf = l\n",
    "                    full_path_to_doc_to_open_txt = full_path_to_doc_to_open_pdf.replace('.pdf', '.txt')\n",
    "\n",
    "                    # replace Drive location\n",
    "    #                 full_path_to_doc_to_open_pdf = full_path_to_doc_to_open_pdf.replace(r'E:', r'D:')\n",
    "    #                 full_path_to_doc_to_open_txt = full_path_to_doc_to_open_txt.replace(r'E:', r'D:')\n",
    "\n",
    "        if cnt > 0:\n",
    "            print(doc_year, full_path_to_doc_to_open_pdf)\n",
    "#             print(doc_year, '\\n', full_path_to_doc_to_open_pdf, '\\n', full_path_to_doc_to_open_txt)\n",
    "            break\n",
    "\n",
    "        doc_year = doc_year + 1\n",
    "\n",
    "    # open pdf document\n",
    "    path_to_pdf = os.path.abspath(full_path_to_doc_to_open_pdf)\n",
    "    path_to_acrobat = os.path.abspath(r'C:\\Program Files (x86)\\Adobe\\Acrobat Reader DC\\Reader\\AcroRd32.exe') \n",
    "    \n",
    "    process = subprocess.Popen([path_to_acrobat, path_to_pdf], shell=False, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderColumns(df_joined, column_list):\n",
    "\n",
    "    # reorder columns\n",
    "\n",
    "    df_new = df_joined[column_list] # reorders columns. Must be the same as the existing column names\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeNamesInDFToReflectCorrectedNames(dict_type, dfToChange):\n",
    "    \n",
    "    # get document type 3X|3Y|3Z\n",
    "    res = list(dict_type.keys())[0]\n",
    "    docType = dict_type[res]['doc_type']\n",
    "\n",
    "    df_filtered_for_doc_type = dfToChange[dfToChange['document_type'] == docType]\n",
    "\n",
    "    for k, v in tqdm_notebook(dict_type.items(), total=len(dict_type), desc='main_loop'):\n",
    "\n",
    "        filename_ = k\n",
    "        \n",
    "        for k1, v1 in v['directors'].items():\n",
    "            \n",
    "            director_name_round_three = k1\n",
    "            director_name_round_four = v1\n",
    "            \n",
    "            # only if director_name_round_three and director_name_round_four are different do you go further\n",
    "            if director_name_round_three != director_name_round_four:\n",
    "\n",
    "                df_temp = df_filtered_for_doc_type[df_filtered_for_doc_type['filename'] == filename_]\n",
    "                \n",
    "                df_temp_director_three = df_temp[df_temp['director_name_round_three'] == director_name_round_three]\n",
    "                \n",
    "                index_list = df_temp_director_three.index.tolist()\n",
    "                \n",
    "                director_name_round_four_check_list = list(set(list(df_temp_director_three['director_name_round_four'])))\n",
    "                \n",
    "                if director_name_round_four_check_list[0] != v1:\n",
    "                    \n",
    "#                     if v1 == 'Peter Sisley Thomas':\n",
    "                        \n",
    "#                         print(k, index_list, director_name_round_four_check_list[0])\n",
    "                    \n",
    "                    docTypeIndex_dict = {'3X': 1, '3Y': 2, '3Z': 3}\n",
    "                    \n",
    "                    index_list = df_temp_director_three.index.tolist()\n",
    "\n",
    "                    for i in range(0, len(index_list)):\n",
    "\n",
    "                        index_ = index_list[i]\n",
    "\n",
    "                        dfToChange.at[index_,'director_name_temp'] = director_name_round_four\n",
    "                        dfToChange.at[index_,'director_name_corrected'] = docTypeIndex_dict[docType]\n",
    "                                    \n",
    "    return dfToChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_cell_string_to_date_object(cell_value):\n",
    "    \n",
    "    from datetime import date\n",
    "    \n",
    "    if not if_nan_return_True_otherwise_return_False(cell_value):\n",
    "    \n",
    "        dateObject = pd.to_datetime(cell_value).strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        dateObject = datetime.strptime(dateObject, '%d/%m/%Y').date()\n",
    "\n",
    "        return dateObject\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return date.today()\n",
    "#         return cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_cell_string_to_date_object_npnan_if_false(cell_value):\n",
    "    \n",
    "    from datetime import date\n",
    "    \n",
    "    if not if_nan_return_True_otherwise_return_False(cell_value):\n",
    "    \n",
    "        dateObject = pd.to_datetime(cell_value).strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        dateObject = datetime.strptime(dateObject, '%d/%m/%Y').date()\n",
    "\n",
    "        return dateObject\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return np.nan\n",
    "#         return cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performDataCheckUsingDirectorName(df_to_check, dir_name):\n",
    "\n",
    "    df_test = df_to_check[df_to_check['director_name_round_four'] == dir_name]\n",
    "\n",
    "    filename_list = list(set(list(df_test['filename'])))\n",
    "    ticker_list = list(set(list(df_test['ticker'])))\n",
    "    company_name_list = list(set(list(df_test['name_to_display'])))\n",
    "    \n",
    "    return filename_list, ticker_list, company_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_nan_return_True_otherwise_return_False(x):\n",
    "    \n",
    "    if (x is np.nan or x != x):\n",
    "        return True\n",
    "    elif (isinstance(x, str) and (x.lower() == 'nan' or x.lower() == 'nat')):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>rec_id</th>\n",
       "      <th>file_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>rec_type0</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>received_datetime</th>\n",
       "      <th>released_datetime</th>\n",
       "      <th>number_of_entities</th>\n",
       "      <th>exchange_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>director_name</th>\n",
       "      <th>last_notice_date</th>\n",
       "      <th>cover_sheet_date</th>\n",
       "      <th>holder_name</th>\n",
       "      <th>interest_type</th>\n",
       "      <th>appointment_date</th>\n",
       "      <th>change_date</th>\n",
       "      <th>cessation_date</th>\n",
       "      <th>event_date</th>\n",
       "      <th>security_name_long</th>\n",
       "      <th>security_name_short</th>\n",
       "      <th>number</th>\n",
       "      <th>prior_number</th>\n",
       "      <th>acquired_number</th>\n",
       "      <th>disposed_number</th>\n",
       "      <th>after_number</th>\n",
       "      <th>change_nature</th>\n",
       "      <th>consideration</th>\n",
       "      <th>consideration_per_security</th>\n",
       "      <th>exercise_price</th>\n",
       "      <th>expiry_date</th>\n",
       "      <th>load_id</th>\n",
       "      <th>date_added</th>\n",
       "      <th>datetime</th>\n",
       "      <th>status</th>\n",
       "      <th>description</th>\n",
       "      <th>_merge_3X3Y3Z</th>\n",
       "      <th>_merge_load_log</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>director_name_string</th>\n",
       "      <th>director_name_changed</th>\n",
       "      <th>director_name_changed_bool</th>\n",
       "      <th>ticker_reviewed_bool</th>\n",
       "      <th>entity_name_changed</th>\n",
       "      <th>entity_name_changed_bool</th>\n",
       "      <th>entity_name_reviewed_bool</th>\n",
       "      <th>director_name_final</th>\n",
       "      <th>director_name_final_bool</th>\n",
       "      <th>director_name_overall</th>\n",
       "      <th>director_name_overall_bool</th>\n",
       "      <th>alternative_name</th>\n",
       "      <th>director_name_round_two</th>\n",
       "      <th>director_name_round_two_bool</th>\n",
       "      <th>director_name_round_one</th>\n",
       "      <th>director_name_round_one_bool</th>\n",
       "      <th>entity_name_corrected</th>\n",
       "      <th>entity_name_final</th>\n",
       "      <th>entity_name_final_ticker_checked</th>\n",
       "      <th>entity_name_final_bool</th>\n",
       "      <th>orig_index_I</th>\n",
       "      <th>ASX_change_bool</th>\n",
       "      <th>ticker_changes</th>\n",
       "      <th>name_changes</th>\n",
       "      <th>last_ticker</th>\n",
       "      <th>last_name</th>\n",
       "      <th>ASX_change_dates</th>\n",
       "      <th>last_ASX_change_date</th>\n",
       "      <th>ticker_to_display</th>\n",
       "      <th>name_to_display</th>\n",
       "      <th>delisted_bool</th>\n",
       "      <th>delisted_date</th>\n",
       "      <th>orig_index_II</th>\n",
       "      <th>director_name_round_three</th>\n",
       "      <th>director_name_round_three_bool</th>\n",
       "      <th>director_name_round_four_old</th>\n",
       "      <th>director_name_round_four_bool</th>\n",
       "      <th>corrected_names_14Nov19_bool</th>\n",
       "      <th>director_name_round_four</th>\n",
       "      <th>director_name_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470275.pdf</td>\n",
       "      <td>319266</td>\n",
       "      <td>70889.0</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>3X</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-11-23 08:25:00</td>\n",
       "      <td>2009-11-23 08:25:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EMS</td>\n",
       "      <td>Eastland Medical Systems Ltd</td>\n",
       "      <td>Calvin John ROSS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2009-11-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-11-20</td>\n",
       "      <td>Ordinary fully paid shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 07:23:38</td>\n",
       "      <td>2019-07-05 07:20:56</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>470275.pdf31926670889.03X</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Eastland Medical Systems Ltd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>EASTLAND MEDICAL SYSTEMS LIMITED</td>\n",
       "      <td>EASTLAND MEDICAL SYSTEMS LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1173</td>\n",
       "      <td>1</td>\n",
       "      <td>EMS, SUD, SUD, SUD</td>\n",
       "      <td>EASTLAND MEDICAL SYSTEMS LIMITED, SUDA LIMITED...</td>\n",
       "      <td>SUD</td>\n",
       "      <td>SUDA PHARMACEUTICALS LIMITED</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>SUD</td>\n",
       "      <td>SUDA PHARMACEUTICALS LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1173</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Calvin John Ross</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6508702.pdf</td>\n",
       "      <td>445323</td>\n",
       "      <td>99694.0</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3X</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-09-29 19:01:00</td>\n",
       "      <td>2011-09-29 19:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SHC</td>\n",
       "      <td>Sunshine Heart, Inc.</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-09-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-09-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:27:05</td>\n",
       "      <td>2019-07-05 07:22:12</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>6508702.pdf44532399694.03X</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunshine Heart Inc</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>SUNSHINE HEART INC</td>\n",
       "      <td>SUNSHINE HEART INC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>229485</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>SHC_</td>\n",
       "      <td>SUNSHINE HEART INC</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-06 00:00:00</td>\n",
       "      <td>229471</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Harvey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6513462.pdf</td>\n",
       "      <td>461688</td>\n",
       "      <td>103389.0</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>3X</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-04 11:09:00</td>\n",
       "      <td>2011-10-04 11:09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>XAM</td>\n",
       "      <td>XANADU MINES LTD</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:52:22</td>\n",
       "      <td>2019-07-05 07:16:23</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>6513462.pdf461688103389.03X</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>XANADU MINES LTD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>XANADU MINES LIMITED</td>\n",
       "      <td>XANADU MINES LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>259059</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>XAM</td>\n",
       "      <td>XANADU MINES LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259043</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hannah Louise Badenach</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01974250.pdf</td>\n",
       "      <td>442406</td>\n",
       "      <td>99059.0</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>3X</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-24 15:58:06</td>\n",
       "      <td>2018-04-24 15:58:06</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>MDD</td>\n",
       "      <td>MANDALONG RESOURCES LIMITED</td>\n",
       "      <td>HONGJUN CUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:22:55</td>\n",
       "      <td>2019-07-05 07:18:50</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>01974250.pdf44240699059.03X</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MANDALONG RESOURCES LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>MANDALONG RESOURCES LIMITED</td>\n",
       "      <td>MANDALONG RESOURCES LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224163</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>MDD_</td>\n",
       "      <td>MANDALONG RESOURCES LIMITED</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-09-27 00:00:00</td>\n",
       "      <td>224150</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hongjun Cui</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>595838.pdf</td>\n",
       "      <td>322981</td>\n",
       "      <td>71694.0</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>3X</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-07-09 13:34:29</td>\n",
       "      <td>2012-07-09 13:34:29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>AQQ</td>\n",
       "      <td>Aphrodite Gold Limited</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-07-09</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 07:29:21</td>\n",
       "      <td>2019-07-05 07:19:50</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>595838.pdf32298171694.03X</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Aphrodite Gold Limited</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>APHRODITE GOLD LIMITED</td>\n",
       "      <td>APHRODITE GOLD LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7957</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>AQQ_</td>\n",
       "      <td>APHRODITE GOLD LIMITED</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-08 00:00:00</td>\n",
       "      <td>7957</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul Maurice Weston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename  pdf_id    rec_id  file_date document_type  rec_type0  \\\n",
       "0    470275.pdf  319266   70889.0 2019-02-06            3X       2008   \n",
       "1   6508702.pdf  445323   99694.0 2019-02-07            3X       2008   \n",
       "2   6513462.pdf  461688  103389.0 2019-07-05            3X       2008   \n",
       "3  01974250.pdf  442406   99059.0 2019-02-05            3X       2008   \n",
       "4    595838.pdf  322981   71694.0 2019-07-05            3X       2008   \n",
       "\n",
       "   rec_type   received_datetime   released_datetime  number_of_entities  \\\n",
       "0       NaN 2009-11-23 08:25:00 2009-11-23 08:25:00                   1   \n",
       "1       NaN 2011-09-29 19:01:00 2011-09-29 19:01:00                   1   \n",
       "2       NaN 2011-10-04 11:09:00 2011-10-04 11:09:00                   1   \n",
       "3       NaN 2018-04-24 15:58:06 2018-04-24 15:58:06                   1   \n",
       "4       NaN 2012-07-09 13:34:29 2012-07-09 13:34:29                   1   \n",
       "\n",
       "   exchange_id ticker                   entity_name           director_name  \\\n",
       "0            0    EMS  Eastland Medical Systems Ltd        Calvin John ROSS   \n",
       "1            0    SHC          Sunshine Heart, Inc.             Mark Harvey   \n",
       "2            0    XAM              XANADU MINES LTD  Hannah Louise Badenach   \n",
       "3            6    MDD   MANDALONG RESOURCES LIMITED             HONGJUN CUI   \n",
       "4            0    AQQ        Aphrodite Gold Limited     Paul Maurice Weston   \n",
       "\n",
       "  last_notice_date cover_sheet_date          holder_name interest_type  \\\n",
       "0              NaN              NaT     Calvin John Ross        Direct   \n",
       "1              NaN              NaT                  NaN           NaN   \n",
       "2              NaN       2011-10-04                  NaN           NaN   \n",
       "3              NaN              NaT                  NaN           NaN   \n",
       "4              NaN       2012-07-09  Paul Maurice Weston        Direct   \n",
       "\n",
       "  appointment_date change_date cessation_date  event_date  \\\n",
       "0       2009-11-20         NaN            NaN  2009-11-20   \n",
       "1       2011-09-29         NaN            NaN  2011-09-29   \n",
       "2       2011-10-04         NaN            NaN  2011-10-04   \n",
       "3       2018-04-24         NaN            NaN  2018-04-24   \n",
       "4       2012-07-06         NaN            NaN  2012-07-06   \n",
       "\n",
       "           security_name_long security_name_short     number prior_number  \\\n",
       "0  Ordinary fully paid shares              Shares  1500000.0            0   \n",
       "1                         NaN                 NaN        0.0            0   \n",
       "2                         NaN                 NaN        0.0          0.0   \n",
       "3                         NaN                 NaN        0.0            0   \n",
       "4             Ordinary Shares              Shares   150000.0            0   \n",
       "\n",
       "  acquired_number  disposed_number  after_number change_nature  consideration  \\\n",
       "0               0              0.0           0.0             0            0.0   \n",
       "1               0              0.0           0.0             0            0.0   \n",
       "2             0.0              0.0           0.0             0            0.0   \n",
       "3               0              0.0           0.0             0            0.0   \n",
       "4               0              0.0           0.0             0            0.0   \n",
       "\n",
       "   consideration_per_security  exercise_price expiry_date  \\\n",
       "0                         0.0             NaN         NaN   \n",
       "1                         0.0             NaN         NaN   \n",
       "2                         0.0             NaN         NaN   \n",
       "3                         0.0             NaN         NaN   \n",
       "4                         0.0             NaN         NaN   \n",
       "\n",
       "                                load_id          date_added  \\\n",
       "0  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 07:23:38   \n",
       "1  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:27:05   \n",
       "2  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:52:22   \n",
       "3  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:22:55   \n",
       "4  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 07:29:21   \n",
       "\n",
       "             datetime   status description _merge_3X3Y3Z _merge_load_log  \\\n",
       "0 2019-07-05 07:20:56  success         NaN          both            both   \n",
       "1 2019-07-05 07:22:12  success         NaN          both            both   \n",
       "2 2019-07-05 07:16:23  success         NaN          both            both   \n",
       "3 2019-07-05 07:18:50  success         NaN          both            both   \n",
       "4 2019-07-05 07:19:50  success         NaN          both            both   \n",
       "\n",
       "                     unique_id    director_name_string  \\\n",
       "0    470275.pdf31926670889.03X        Calvin John Ross   \n",
       "1   6508702.pdf44532399694.03X             Mark Harvey   \n",
       "2  6513462.pdf461688103389.03X  Hannah Louise Badenach   \n",
       "3  01974250.pdf44240699059.03X             Hongjun Cui   \n",
       "4    595838.pdf32298171694.03X     Paul Maurice Weston   \n",
       "\n",
       "    director_name_changed  director_name_changed_bool  ticker_reviewed_bool  \\\n",
       "0        Calvin John Ross                           0                     1   \n",
       "1             Mark Harvey                           0                     1   \n",
       "2  Hannah Louise Badenach                           0                     1   \n",
       "3             Hongjun Cui                           0                     1   \n",
       "4     Paul Maurice Weston                           0                     1   \n",
       "\n",
       "            entity_name_changed  entity_name_changed_bool  \\\n",
       "0  Eastland Medical Systems Ltd                         0   \n",
       "1            Sunshine Heart Inc                         1   \n",
       "2              XANADU MINES LTD                         0   \n",
       "3   MANDALONG RESOURCES LIMITED                         0   \n",
       "4        Aphrodite Gold Limited                         0   \n",
       "\n",
       "   entity_name_reviewed_bool     director_name_final  \\\n",
       "0                          0        Calvin John Ross   \n",
       "1                          0             Mark Harvey   \n",
       "2                          0  Hannah Louise Badenach   \n",
       "3                          0             Hongjun Cui   \n",
       "4                          0     Paul Maurice Weston   \n",
       "\n",
       "   director_name_final_bool   director_name_overall  \\\n",
       "0                         0        Calvin John Ross   \n",
       "1                         0             Mark Harvey   \n",
       "2                         0  Hannah Louise Badenach   \n",
       "3                         0             Hongjun Cui   \n",
       "4                         0     Paul Maurice Weston   \n",
       "\n",
       "   director_name_overall_bool alternative_name director_name_round_two  \\\n",
       "0                           0              NaN        Calvin John Ross   \n",
       "1                           0              NaN             Mark Harvey   \n",
       "2                           0              NaN  Hannah Louise Badenach   \n",
       "3                           0              NaN             Hongjun Cui   \n",
       "4                           0              NaN     Paul Maurice Weston   \n",
       "\n",
       "   director_name_round_two_bool director_name_round_one  \\\n",
       "0                             0        Calvin John Ross   \n",
       "1                             0             Mark Harvey   \n",
       "2                             0  Hannah Louise Badenach   \n",
       "3                             0             Hongjun Cui   \n",
       "4                             0     Paul Maurice Weston   \n",
       "\n",
       "   director_name_round_one_bool             entity_name_corrected  \\\n",
       "0                             0  EASTLAND MEDICAL SYSTEMS LIMITED   \n",
       "1                             0                SUNSHINE HEART INC   \n",
       "2                             0              XANADU MINES LIMITED   \n",
       "3                             0       MANDALONG RESOURCES LIMITED   \n",
       "4                             0            APHRODITE GOLD LIMITED   \n",
       "\n",
       "                  entity_name_final  entity_name_final_ticker_checked  \\\n",
       "0  EASTLAND MEDICAL SYSTEMS LIMITED                                 0   \n",
       "1                SUNSHINE HEART INC                                 0   \n",
       "2              XANADU MINES LIMITED                                 0   \n",
       "3       MANDALONG RESOURCES LIMITED                                 0   \n",
       "4            APHRODITE GOLD LIMITED                                 0   \n",
       "\n",
       "   entity_name_final_bool  orig_index_I  ASX_change_bool      ticker_changes  \\\n",
       "0                       0          1173                1  EMS, SUD, SUD, SUD   \n",
       "1                       0        229485                0                 NaN   \n",
       "2                       0        259059                0                 NaN   \n",
       "3                       0        224163                0                 NaN   \n",
       "4                       0          7957                0                 NaN   \n",
       "\n",
       "                                        name_changes last_ticker  \\\n",
       "0  EASTLAND MEDICAL SYSTEMS LIMITED, SUDA LIMITED...         SUD   \n",
       "1                                                NaN         NaN   \n",
       "2                                                NaN         NaN   \n",
       "3                                                NaN         NaN   \n",
       "4                                                NaN         NaN   \n",
       "\n",
       "                      last_name ASX_change_dates last_ASX_change_date  \\\n",
       "0  SUDA PHARMACEUTICALS LIMITED              NaT           2017-12-05   \n",
       "1                           NaN              NaT                  NaT   \n",
       "2                           NaN              NaT                  NaT   \n",
       "3                           NaN              NaT                  NaT   \n",
       "4                           NaN              NaT                  NaT   \n",
       "\n",
       "  ticker_to_display               name_to_display  delisted_bool  \\\n",
       "0               SUD  SUDA PHARMACEUTICALS LIMITED              0   \n",
       "1              SHC_            SUNSHINE HEART INC              1   \n",
       "2               XAM          XANADU MINES LIMITED              0   \n",
       "3              MDD_   MANDALONG RESOURCES LIMITED              1   \n",
       "4              AQQ_        APHRODITE GOLD LIMITED              1   \n",
       "\n",
       "         delisted_date  orig_index_II director_name_round_three  \\\n",
       "0                  NaN           1173          Calvin John Ross   \n",
       "1  2013-05-06 00:00:00         229471               Mark Harvey   \n",
       "2                  NaN         259043    Hannah Louise Badenach   \n",
       "3  2018-09-27 00:00:00         224150               Hongjun Cui   \n",
       "4  2018-01-08 00:00:00           7957       Paul Maurice Weston   \n",
       "\n",
       "   director_name_round_three_bool director_name_round_four_old  \\\n",
       "0                               0             Calvin John Ross   \n",
       "1                               0                  Mark Harvey   \n",
       "2                               0       Hannah Louise Badenach   \n",
       "3                               0                  Hongjun Cui   \n",
       "4                               0          Paul Maurice Weston   \n",
       "\n",
       "   director_name_round_four_bool  corrected_names_14Nov19_bool  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              0                             0   \n",
       "3                              0                             0   \n",
       "4                              0                             0   \n",
       "\n",
       "  director_name_round_four  director_name_corrected  \n",
       "0         Calvin John Ross                        0  \n",
       "1              Mark Harvey                        0  \n",
       "2   Hannah Louise Badenach                        0  \n",
       "3              Hongjun Cui                        0  \n",
       "4      Paul Maurice Weston                        0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_dates = open_pickle_file_as_df(df_GRAND_SOURCE)\n",
    "df_check_dates.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df_check_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EMB'], ['EMBELTON LIMITED'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performDataCheckUsingDirectorName(df_check_dates, 'John M Harrison')[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop through df and check dates - first main dictionary generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "check_dates_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_temp = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    director_name_list = list(set(list(df_temp['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elex in enumerate(director_name_list):\n",
    "        \n",
    "        df_temp_director = df_temp[df_temp['director_name_round_four'] == elex]\n",
    "        \n",
    "        index_error_list = [] \n",
    "        \n",
    "        if df_temp_director.shape[0] > 0:\n",
    "            \n",
    "            index_list = df_temp_director.index.tolist()\n",
    "            \n",
    "            index_list_sorted = sorted(index_list)\n",
    "            \n",
    "            first_row = index_list_sorted[0]\n",
    "            \n",
    "            cover_sheet_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cover_sheet_date'])\n",
    "            last_notice_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'last_notice_date'])\n",
    "            appointment_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'appointment_date'])\n",
    "            cessation_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cessation_date'])\n",
    "            change_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'change_date'])\n",
    "            event_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'event_date'])\n",
    "            \n",
    "            first_row_date_list = [\n",
    "                cover_sheet_date,\n",
    "                last_notice_date,\n",
    "                appointment_date,\n",
    "                cessation_date,\n",
    "                change_date,\n",
    "                event_date\n",
    "            ]\n",
    "            \n",
    "            for idxxx, elexxx in enumerate(index_list_sorted):\n",
    "                \n",
    "                if idxxx > 0:\n",
    "                    \n",
    "                    cover_sheet_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'cover_sheet_date'])\n",
    "                    last_notice_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'last_notice_date'])\n",
    "                    appointment_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'appointment_date'])\n",
    "                    cessation_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'cessation_date'])\n",
    "                    change_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'change_date'])\n",
    "                    event_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'event_date'])\n",
    "                    \n",
    "                    subsequent_row_date_list = [\n",
    "                        cover_sheet_date,\n",
    "                        last_notice_date,\n",
    "                        appointment_date,\n",
    "                        cessation_date,\n",
    "                        change_date,\n",
    "                        event_date\n",
    "                    ]\n",
    "                    \n",
    "                    for i in range(0, len(first_row_date_list)):\n",
    "                        \n",
    "                        director_name_error_list = []\n",
    "                        \n",
    "                        sub_director_dict = {}\n",
    "                        \n",
    "                        if first_row_date_list[i] != subsequent_row_date_list[i]:\n",
    "                            \n",
    "                            index_error_list.append(first_row)\n",
    "                            index_error_list.append(elexxx)\n",
    "                            index_error_list = list(set(index_error_list))\n",
    "                            \n",
    "#                             print(first_row_date_list[i], subsequent_row_date_list[i])\n",
    "                            \n",
    "                            key = str(ele) + r'_' + str(elex)\n",
    "                            \n",
    "                            director_name_error_list.append(elex)\n",
    "                            \n",
    "                            director_name_error_list = list(set(director_name_error_list))\n",
    "                            \n",
    "                            if key not in check_dates_dict:\n",
    "                                \n",
    "                                check_dates_dict.setdefault(key, [])\n",
    "                                \n",
    "                            sub_director_dict.setdefault(first_row, [])\n",
    "                            \n",
    "                            sub_director_dict[first_row] = elex\n",
    "                            \n",
    "                            index_error_list_sorted = sorted(index_error_list)\n",
    "                            \n",
    "                            # get doc_types\n",
    "                            \n",
    "                            df_doc_types = df_check_dates[\n",
    "                                (df_check_dates['filename'] == ele)\n",
    "                                &\n",
    "                                (df_check_dates['director_name_round_four'] == elex)\n",
    "                                &\n",
    "                                (df_check_dates.index.isin(index_error_list_sorted))\n",
    "\n",
    "                            ]\n",
    "                            \n",
    "                            doc_types = list(set(list(df_doc_types['document_type'])))\n",
    "                            \n",
    "                            check_dates_dict[key] = {'filename': ele, 'director': sub_director_dict[first_row], 'first_row': first_row, 'complete_list': index_error_list_sorted, 'document_types': doc_types}\n",
    "                            \n",
    "#                             print('different', ele, check_dates_dict[key])\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnme = '232779.pdf'\n",
    "dir_name = 'Paul Lin'\n",
    "row_list = [253, 34295]\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == fnme)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "#     &\n",
    "#     (df_check_dates.index.isin(row_list))\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE date error dictionary\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'date_errors_across_filtered_rows_dict_III_12JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, check_dates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open saved dictionary pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'date_errors_across_filtered_rows_dict_III_12JUL20.pickle'\n",
    "\n",
    "check_dates_dict = open_pickle_file_as_df(pickle_filepath_and_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dates_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. first check if different date is cover sheet date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list = []\n",
    "\n",
    "for k, v in check_dates_dict.items():\n",
    "    \n",
    "    dict_key_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors_list = []\n",
    "\n",
    "for idx, ele in enumerate(dict_key_list):\n",
    "    \n",
    "    detected_error_row_list = []\n",
    "    \n",
    "    filename_ = check_dates_dict[ele]['filename']\n",
    "    director_name = check_dates_dict[ele]['director']\n",
    "    error_row_list = check_dates_dict[ele]['complete_list']\n",
    "    \n",
    "    df_temp = df_check_dates[(df_check_dates['filename'] == filename_)\n",
    "                            &\n",
    "                            (df_check_dates['director_name_round_four'] == director_name)\n",
    "                            &\n",
    "                            (df_check_dates.index.isin(error_row_list))\n",
    "                            ]\n",
    "    \n",
    "    first_row = error_row_list[0]\n",
    "    \n",
    "    cover_sheet_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cover_sheet_date'])\n",
    "    \n",
    "    for idxx, elex in enumerate(error_row_list):\n",
    "        \n",
    "        if idxx > 0:\n",
    "            \n",
    "            cover_sheet_date_subs_row = convert_df_cell_string_to_date_object(df_check_dates.loc[elex, 'cover_sheet_date'])\n",
    "            \n",
    "            if cover_sheet_date_first_row != cover_sheet_date_subs_row:\n",
    "                \n",
    "                detected_error_row_list.append(first_row)\n",
    "                detected_error_row_list.append(elex)\n",
    "                detected_error_row_list = list(set(detected_error_row_list))\n",
    "                \n",
    "    if len(detected_error_row_list) > 0:\n",
    "\n",
    "        print(ele, filename_, director_name, detected_error_row_list) #first_row, elex)\n",
    "        \n",
    "        temp_list = [ele, filename_, director_name, detected_error_row_list]\n",
    "        \n",
    "        all_errors_list.append(temp_list)\n",
    "            \n",
    "print('analysis completed')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_use = 0\n",
    "to_use_list = all_errors_list[index_to_use]\n",
    "to_use_list[1], to_use_list[2], to_use_list[3], len(all_errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ = to_use_list[1]\n",
    "director_name = to_use_list[2]\n",
    "error_row_list = to_use_list[3]\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == director_name)\n",
    "    &\n",
    "    (df_check_dates.index.isin(error_row_list))\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_correct = 10016\n",
    "cover_sheet_date_to_use = '2010-09-01'\n",
    "\n",
    "if df_check_dates.loc[index_to_correct, 'director_name_round_four'] == director_name:\n",
    "\n",
    "    df_check_dates.at[index_to_correct, 'cover_sheet_date'] = cover_sheet_date_to_use\n",
    "    \n",
    "    print('change made for', filename_, director_name, index_to_correct)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('THERE IS AN ERROR!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. second check, check all other dates, but split by doc type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors_list = []\n",
    "\n",
    "date_type_dict = {0: 'cover_sheet_date',\n",
    "                  1: 'last_notice_date',\n",
    "                  2: 'appointment_date',\n",
    "                  3: 'change_date',\n",
    "                  4: 'cessation_date',\n",
    "                  5: 'event_date'}\n",
    "\n",
    "for idx, ele in enumerate(dict_key_list):\n",
    "    \n",
    "    filename_ = check_dates_dict[ele]['filename']\n",
    "    director_name = check_dates_dict[ele]['director']\n",
    "    error_row_list = check_dates_dict[ele]['complete_list']\n",
    "    doc_type_list = check_dates_dict[ele]['document_types']\n",
    "    \n",
    "    for idxx, elex in enumerate(doc_type_list):\n",
    "        \n",
    "        error_row_detected_list = []\n",
    "        date_error_type = []\n",
    "    \n",
    "        df_temp = df_check_dates[(df_check_dates['filename'] == filename_)\n",
    "                                &\n",
    "                                (df_check_dates['director_name_round_four'] == director_name)\n",
    "                                &\n",
    "                                (df_check_dates['document_type'] == elex)\n",
    "                                &\n",
    "                                (df_check_dates.index.isin(error_row_list))\n",
    "                                ]\n",
    "\n",
    "        if df_temp.shape[0] > 1:\n",
    "            \n",
    "#             print(idxx, elex)\n",
    "            \n",
    "            temp_row_list = df_temp.index.tolist()\n",
    "            temp_row_list_sorted = sorted(temp_row_list)\n",
    "        \n",
    "            first_row = temp_row_list_sorted[0]\n",
    "\n",
    "            cover_sheet_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cover_sheet_date'])\n",
    "            last_notice_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'last_notice_date'])\n",
    "            appointment_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'appointment_date'])\n",
    "            cessation_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cessation_date'])\n",
    "            change_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'change_date'])\n",
    "            event_date = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'event_date'])\n",
    "\n",
    "            first_row_date_list = [\n",
    "                cover_sheet_date,\n",
    "                last_notice_date,\n",
    "                appointment_date,\n",
    "                cessation_date,\n",
    "                change_date,\n",
    "                event_date\n",
    "            ]\n",
    "\n",
    "            for idxxx, elexx in enumerate(temp_row_list_sorted):\n",
    "\n",
    "                if idxxx > 0:\n",
    "                    \n",
    "#                     print(idxx, elex)\n",
    "\n",
    "                    cover_sheet_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'cover_sheet_date'])\n",
    "                    last_notice_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'last_notice_date'])\n",
    "                    appointment_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'appointment_date'])\n",
    "                    cessation_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'cessation_date'])\n",
    "                    change_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'change_date'])\n",
    "                    event_date = convert_df_cell_string_to_date_object(df_check_dates.loc[elexx, 'event_date'])\n",
    "\n",
    "                    subsequent_row_date_list = [\n",
    "                        cover_sheet_date,\n",
    "                        last_notice_date,\n",
    "                        appointment_date,\n",
    "                        cessation_date,\n",
    "                        change_date,\n",
    "                        event_date\n",
    "                    ]\n",
    "\n",
    "                    for idxxxx, elexxx in enumerate(first_row_date_list):\n",
    "\n",
    "                        if first_row_date_list[idxxxx] != subsequent_row_date_list[idxxxx]:\n",
    "\n",
    "                            error_row_detected_list.append(first_row)\n",
    "                            error_row_detected_list.append(elexx)\n",
    "                            error_row_detected_list = list(set(error_row_detected_list))\n",
    "\n",
    "                            error_row_detected_list_sorted = sorted(error_row_detected_list)\n",
    "                            \n",
    "                            date_error_type.append(date_type_dict[idxxxx])\n",
    "                            date_error_type = list(set(date_error_type))\n",
    "\n",
    "    if len(error_row_detected_list) > 0:\n",
    "\n",
    "        print(ele, filename_, director_name, error_row_detected_list, date_error_type) #first_row, elex)\n",
    "        \n",
    "        temp_list = [ele, filename_, director_name, error_row_detected_list, date_error_type]\n",
    "        \n",
    "        all_errors_list.append(temp_list)\n",
    "            \n",
    "print('analysis completed')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_use = 0\n",
    "to_use_list = all_errors_list[index_to_use]\n",
    "to_use_list[1], to_use_list[2], to_use_list[3], to_use_list[4], len(all_errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename_ = to_use_list[1]\n",
    "director_name = to_use_list[2]\n",
    "error_row_list = to_use_list[3]\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == director_name)\n",
    "    &\n",
    "    (df_check_dates.index.isin(error_row_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_type_dict = {0: 'cover_sheet_date',\n",
    "                  1: 'last_notice_date',\n",
    "                  2: 'appointment_date',\n",
    "                  3: 'change_date',\n",
    "                  4: 'cessation_date',\n",
    "                  5: 'event_date'}\n",
    "\n",
    "index_to_correct = 33128\n",
    "date_to_use = '2011-08-10'\n",
    "\n",
    "if df_check_dates.loc[index_to_correct, 'director_name_round_four'] == director_name:\n",
    "\n",
    "    df_check_dates.at[index_to_correct, 'event_date'] = date_to_use\n",
    "    \n",
    "    print('change made for', filename_, director_name, index_to_correct, 'to', date_to_use)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('THERE IS AN ERROR!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save corrected df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE date error dictionary\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_12JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do:\n",
    "1. run first dictionary generator to check work - done\n",
    "2. filter for filename only and check for cover sheet date uniformity\n",
    "3. filter for 3X & 3Z and check event date is not nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. filter for filename only and check for cover sheet date uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "check_dates_filename_and_coversheet_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_temp = df_check_dates[df_check_dates['filename'] == ele]\n",
    "        \n",
    "    index_error_list = []\n",
    "\n",
    "    if df_temp.shape[0] > 0:\n",
    "\n",
    "        index_list = df_temp.index.tolist()\n",
    "\n",
    "        index_list_sorted = sorted(index_list)\n",
    "\n",
    "        first_row = index_list_sorted[0]\n",
    "\n",
    "        cover_sheet_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cover_sheet_date'])\n",
    "\n",
    "        for idxxx, elexxx in enumerate(index_list_sorted):\n",
    "\n",
    "            if idxxx > 0:\n",
    "\n",
    "                cover_sheet_date_subs_row = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'cover_sheet_date'])\n",
    "\n",
    "                if cover_sheet_date_first_row != cover_sheet_date_subs_row:\n",
    "\n",
    "#                     index_error_list.append(first_row)\n",
    "#                     index_error_list.append(elexxx)\n",
    "                    for x in index_list:\n",
    "                        index_error_list.append(x)\n",
    "                    \n",
    "                    index_error_list = list(set(index_error_list))\n",
    "\n",
    "#                             print(first_row_date_list[i], subsequent_row_date_list[i])\n",
    "\n",
    "                    key = ele\n",
    "\n",
    "                    if key not in check_dates_filename_and_coversheet_dict:\n",
    "\n",
    "                        check_dates_filename_and_coversheet_dict.setdefault(key, [])\n",
    "\n",
    "                    index_error_list_sorted = sorted(index_error_list)\n",
    "\n",
    "                    check_dates_filename_and_coversheet_dict[key] = {'filename': ele, 'first_row': first_row, 'complete_list': index_error_list_sorted}\n",
    "\n",
    "#                             print('different', ele, check_dates_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dates_filename_and_coversheet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_dates_filename_and_coversheet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list = []\n",
    "\n",
    "for k, v in check_dates_filename_and_coversheet_dict.items():\n",
    "    \n",
    "    dict_key_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_use = 0\n",
    "filename_ = dict_key_list[index_to_use]\n",
    "error_row_list = check_dates_filename_and_coversheet_dict[filename_]['complete_list']\n",
    "filename_, error_row_list, len(dict_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates.index.isin(error_row_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_correct_list = [29895, 29896]\n",
    "index_zero = index_to_correct_list[0]\n",
    "date_to_use = '2009-02-12'\n",
    "\n",
    "# test that all rows have the correct filename\n",
    "correct_row_bool_list = []\n",
    "\n",
    "for i in range(0, len(index_to_correct_list)):\n",
    "    \n",
    "    if df_check_dates.loc[index_to_correct_list[i], 'filename'] == filename_:\n",
    "        \n",
    "        correct_row_bool_list.append(True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        correct_row_bool_list.append(True)\n",
    "        \n",
    "correct_row_bool_list = list(set(correct_row_bool_list))\n",
    "\n",
    "# if df_check_dates.loc[index_zero, 'filename'] == filename_:\n",
    "\n",
    "if len(correct_row_bool_list) == 1 and correct_row_bool_list[0] == True:\n",
    "\n",
    "    for i in range(0, len(index_to_correct_list)):\n",
    "        \n",
    "        df_check_dates.at[index_to_correct_list[i], 'cover_sheet_date'] = date_to_use\n",
    "    \n",
    "        print('change made for', filename_, '; index:', index_to_correct_list[i], '; to:', date_to_use)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('THERE IS AN ERROR!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save corrected df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE date error dictionary\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_13JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. filter for filename  & [3X | 3Z] only and check for existence of event_date and:\n",
    "1. appointment_date == event_date [3X]\n",
    "2. cessation_date == event_date [3Z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "doc_type_list = ['3X','3Z']\n",
    "\n",
    "check_3X_3Z_dates_filename_and_coversheet_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_temp = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    director_name_list = list(set(list(df_temp['director_name_round_four'])))\n",
    "        \n",
    "#     index_error_list = []\n",
    "    \n",
    "    for ii in range(0, len(director_name_list)):\n",
    "        \n",
    "        index_error_list = []\n",
    "        \n",
    "        dir_name = director_name_list[ii]\n",
    "        \n",
    "        df_temp_dir_name = df_temp[df_temp['director_name_round_four'] == dir_name]\n",
    "    \n",
    "        # filter for doc type\n",
    "\n",
    "        for idxx, elexx in enumerate(doc_type_list):\n",
    "\n",
    "            df_temp_doc_type = df_temp_dir_name[df_temp_dir_name['document_type'] == elexx]\n",
    "\n",
    "            if df_temp_doc_type.shape[0] > 0:\n",
    "\n",
    "                index_list = df_temp_doc_type.index.tolist()\n",
    "\n",
    "                index_list_sorted = sorted(index_list)\n",
    "\n",
    "                first_row = index_list_sorted[0]\n",
    "\n",
    "                if elexx == '3X':\n",
    "\n",
    "                    comparison_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'appointment_date'])\n",
    "\n",
    "                elif elexx == '3Z':\n",
    "\n",
    "                    comparison_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cessation_date'])\n",
    "\n",
    "                event_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'event_date'])\n",
    "\n",
    "                first_row_date_list = [\n",
    "                    comparison_date_first_row,\n",
    "                    event_date_first_row\n",
    "                ]\n",
    "\n",
    "                for idxxx, elexxx in enumerate(index_list_sorted):\n",
    "\n",
    "                    if idxxx > 0:\n",
    "\n",
    "                        if elexx == '3X':\n",
    "\n",
    "                            comparison_date_subs_row = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'appointment_date'])\n",
    "\n",
    "                        elif elexx == '3Z':\n",
    "\n",
    "                            comparison_date_subs_row = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'cessation_date'])\n",
    "\n",
    "                        event_date_subs_row = convert_df_cell_string_to_date_object(df_check_dates.loc[elexxx, 'event_date'])\n",
    "\n",
    "                        subsq_row_date_list = [\n",
    "                            comparison_date_subs_row,\n",
    "                            event_date_subs_row\n",
    "                        ]\n",
    "\n",
    "                        for i in range(0, len(first_row_date_list)):\n",
    "\n",
    "                            if (first_row_date_list[i] != subsq_row_date_list[i]) or (comparison_date_first_row != event_date_first_row) or (comparison_date_subs_row != event_date_subs_row):\n",
    "\n",
    "    #                             index_error_list.append(first_row)\n",
    "    #                             index_error_list.append(elexxx)\n",
    "\n",
    "                                for x in index_list_sorted:\n",
    "                                    index_error_list.append(x)\n",
    "\n",
    "                                index_error_list = list(set(index_error_list))\n",
    "\n",
    "            #                             print(first_row_date_list[i], subsequent_row_date_list[i])\n",
    "\n",
    "                                key = str(ele) + r'_' + dir_name.replace(' ', '_')\n",
    "\n",
    "                                if key not in check_3X_3Z_dates_filename_and_coversheet_dict:\n",
    "\n",
    "                                    check_3X_3Z_dates_filename_and_coversheet_dict.setdefault(key, [])\n",
    "\n",
    "                                index_error_list_sorted = sorted(index_error_list)\n",
    "\n",
    "                                check_3X_3Z_dates_filename_and_coversheet_dict[key] = {'filename': ele, 'director_name': dir_name, 'doc_type': elexx, 'complete_list': index_error_list_sorted}\n",
    "                                \n",
    "#                                 if ele == '01792355.pdf':\n",
    "                                    \n",
    "#                                     print(key, check_3X_3Z_dates_filename_and_coversheet_dict[key])\n",
    "\n",
    "#                                 print('difference recorded', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save check_3X_3Z_dates_filename_and_coversheet_dict.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'check_3X_3Z_dates_filename_and_coversheet_dict_14JUL20_b.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, check_3X_3Z_dates_filename_and_coversheet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list = []\n",
    "\n",
    "for k, v in check_3X_3Z_dates_filename_and_coversheet_dict.items():\n",
    "    \n",
    "    dict_key_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list_SHORTER = dict_key_list #[240:254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames_completed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list_SHORTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_completed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start here AGAIN!\n",
    "\n",
    "for idx, elex in enumerate(dict_key_list_SHORTER):\n",
    "    \n",
    "    print(idx, elex)\n",
    "    \n",
    "    if elex not in filenames_completed_list:\n",
    "\n",
    "        index_to_use = idx\n",
    "        key_for_dict = elex\n",
    "        filename_ = check_3X_3Z_dates_filename_and_coversheet_dict[key_for_dict]['filename']\n",
    "        doc_type_in_dict = check_3X_3Z_dates_filename_and_coversheet_dict[key_for_dict]['doc_type']\n",
    "        dir_name = check_3X_3Z_dates_filename_and_coversheet_dict[key_for_dict]['director_name']\n",
    "        error_row_list = check_3X_3Z_dates_filename_and_coversheet_dict[key_for_dict]['complete_list']\n",
    "\n",
    "        index_to_correct_list = error_row_list\n",
    "\n",
    "        doc_type_in_use = '3X'\n",
    "\n",
    "        if doc_type_in_use == doc_type_in_dict:\n",
    "\n",
    "            df_to_get_list_of_dates = df_check_dates[\n",
    "                (df_check_dates['filename'] == filename_)\n",
    "                &\n",
    "                (df_check_dates['document_type'] == doc_type_in_use)\n",
    "                &\n",
    "                (df_check_dates['director_name_round_four'] == dir_name)\n",
    "                &\n",
    "                (df_check_dates.index.isin(error_row_list))\n",
    "            ]\n",
    "\n",
    "            if doc_type_in_use == '3X':\n",
    "\n",
    "                list_of_dates_in_df_list = list(set(list(df_to_get_list_of_dates['appointment_date'])))\n",
    "\n",
    "            else:\n",
    "\n",
    "                list_of_dates_in_df_list = list(set(list(df_to_get_list_of_dates['cessation_date'])))\n",
    "\n",
    "            # get list of dates currently used in df\n",
    "\n",
    "            openPDFFileFromFileName(filename_)\n",
    "\n",
    "            print(filename_, doc_type_in_use, '____', dir_name, '____', error_row_list, idx, len(dict_key_list_SHORTER), len(filenames_completed_list), len(dict_key_list), list_of_dates_in_df_list)\n",
    "\n",
    "            date_to_use = input('What date do you want to use')\n",
    "\n",
    "            substring = r\"^[\\-]{2}\"\n",
    "\n",
    "            if search(substring, date_to_use):\n",
    "                date_to_use = list_of_dates_in_df_list[0]\n",
    "            else:\n",
    "                date_to_use = date_to_use\n",
    "\n",
    "            # test that all rows have the correct filename\n",
    "            \n",
    "            #### AND ADD IN DIRECTOR NAME AS WELL ####\n",
    "            correct_row_bool_list = []\n",
    "\n",
    "            for i in range(0, len(index_to_correct_list)):\n",
    "\n",
    "                if df_check_dates.loc[index_to_correct_list[i], 'filename'] == filename_ and df_check_dates.loc[index_to_correct_list[i], 'director_name_round_four'] == dir_name:\n",
    "\n",
    "                    correct_row_bool_list.append(True)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    correct_row_bool_list.append(True)\n",
    "\n",
    "            correct_row_bool_list = list(set(correct_row_bool_list))\n",
    "\n",
    "            # if df_check_dates.loc[index_zero, 'filename'] == filename_:\n",
    "\n",
    "            if len(correct_row_bool_list) == 1 and correct_row_bool_list[0] == True:\n",
    "                \n",
    "                filenames_completed_list.append(elex)\n",
    "\n",
    "                for i in range(0, len(index_to_correct_list)):\n",
    "\n",
    "                    if doc_type_in_use == '3X':\n",
    "\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'appointment_date'] = date_to_use\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'event_date'] = date_to_use\n",
    "\n",
    "                        print('change made for', filename_, '; director:', dir_name, 'index:', index_to_correct_list[i], '; to:', date_to_use)\n",
    "\n",
    "                    elif doc_type_in_use == '3Z':\n",
    "\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'cessation_date'] = date_to_use\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'event_date'] = date_to_use\n",
    "\n",
    "                        print('change made for', filename_, '; director:', dir_name, 'index:', index_to_correct_list[i], '; to:', date_to_use)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print('THERE IS AN ERROR!!!')\n",
    "            \n",
    "print('all done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 721220.pdf_Alexander_James_Neuling_3Z\n",
    "\n",
    "df_check_dates[\n",
    "#     (df_check_dates['document_type'] == '3Z')\n",
    "#     &\n",
    "    (df_check_dates['filename'] == '721220.pdf')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save corrected df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_14JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. rework No 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "doc_type_list = ['3X','3Z']\n",
    "\n",
    "check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_temp = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    director_name_list = list(set(list(df_temp['director_name_round_four'])))\n",
    "        \n",
    "#     index_error_list = []\n",
    "    \n",
    "    for ii in range(0, len(director_name_list)):\n",
    "        \n",
    "        index_error_list = []\n",
    "        \n",
    "        dir_name = director_name_list[ii]\n",
    "        \n",
    "        df_temp_dir_name = df_temp[df_temp['director_name_round_four'] == dir_name]\n",
    "    \n",
    "        # filter for doc type\n",
    "\n",
    "        for idxx, elexx in enumerate(doc_type_list):\n",
    "\n",
    "            df_temp_doc_type = df_temp_dir_name[df_temp_dir_name['document_type'] == elexx]\n",
    "\n",
    "            if df_temp_doc_type.shape[0] > 0:\n",
    "\n",
    "                index_list = df_temp_doc_type.index.tolist()\n",
    "\n",
    "                index_list_sorted = sorted(index_list)\n",
    "\n",
    "                first_row = index_list_sorted[0]\n",
    "\n",
    "                if elexx == '3X':\n",
    "\n",
    "                    comparison_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'appointment_date'])\n",
    "\n",
    "                elif elexx == '3Z':\n",
    "\n",
    "                    comparison_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'cessation_date'])\n",
    "\n",
    "                event_date_first_row = convert_df_cell_string_to_date_object(df_check_dates.loc[first_row, 'event_date'])\n",
    "\n",
    "                \n",
    "\n",
    "                if (comparison_date_first_row != event_date_first_row) or if_nan_return_True_otherwise_return_False(comparison_date_first_row) or if_nan_return_True_otherwise_return_False(event_date_first_row):\n",
    "\n",
    "#                             index_error_list.append(first_row)\n",
    "#                             index_error_list.append(elexxx)\n",
    "\n",
    "                    for x in index_list_sorted:\n",
    "                        index_error_list.append(x)\n",
    "\n",
    "                    index_error_list = list(set(index_error_list))\n",
    "\n",
    "#                             print(first_row_date_list[i], subsequent_row_date_list[i])\n",
    "\n",
    "                    key = str(ele) + r'_' + dir_name.replace(' ', '_') + r'_' + elexx\n",
    "\n",
    "                    if key not in check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict:\n",
    "\n",
    "                        check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict.setdefault(key, [])\n",
    "\n",
    "                    index_error_list_sorted = sorted(index_error_list)\n",
    "\n",
    "                    check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict[key] = {'filename': ele, 'director_name': dir_name, 'doc_type': elexx, 'complete_list': index_error_list_sorted}\n",
    "\n",
    "#                                 if ele == '01792355.pdf':\n",
    "\n",
    "#                                     print(key, check_3X_3Z_dates_filename_and_coversheet_dict[key])\n",
    "\n",
    "#                     print('difference recorded', key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save rework dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save check_3X_3Z_dates_filename_and_coversheet_dict.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict_14JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REWORK 1 CORRECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list = []\n",
    "\n",
    "for k, v in check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict.items():\n",
    "    \n",
    "    dict_key_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_key_list_SHORTER = dict_key_list #[250:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames_completed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here AGAIN!\n",
    "\n",
    "for idx, elex in enumerate(dict_key_list_SHORTER):\n",
    "    \n",
    "    print(idx, elex)\n",
    "    \n",
    "    if elex not in filenames_completed_list:\n",
    "\n",
    "        index_to_use = idx\n",
    "        key_for_dict = elex\n",
    "        filename_ = check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict[key_for_dict]['filename']\n",
    "        doc_type_in_dict = check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict[key_for_dict]['doc_type']\n",
    "        dir_name = check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict[key_for_dict]['director_name']\n",
    "        error_row_list = check_3X_3Z_dates_filename_and_coversheet_REWORK_1_dict[key_for_dict]['complete_list']\n",
    "\n",
    "        index_to_correct_list = error_row_list\n",
    "\n",
    "        doc_type_in_use = '3Z'\n",
    "\n",
    "        if doc_type_in_use == doc_type_in_dict:\n",
    "\n",
    "            df_to_get_list_of_dates = df_check_dates[\n",
    "                (df_check_dates['filename'] == filename_)\n",
    "                &\n",
    "                (df_check_dates['document_type'] == doc_type_in_use)\n",
    "                &\n",
    "                (df_check_dates['director_name_round_four'] == dir_name)\n",
    "                &\n",
    "                (df_check_dates.index.isin(error_row_list))\n",
    "            ]\n",
    "\n",
    "            if doc_type_in_use == '3X':\n",
    "\n",
    "                list_of_dates_in_df_list = list(set(list(df_to_get_list_of_dates['appointment_date'])))\n",
    "\n",
    "            else:\n",
    "\n",
    "                list_of_dates_in_df_list = list(set(list(df_to_get_list_of_dates['cessation_date'])))\n",
    "\n",
    "            # get list of dates currently used in df\n",
    "\n",
    "            openPDFFileFromFileName(filename_)\n",
    "\n",
    "            print(filename_, doc_type_in_use, '____', dir_name, '____', error_row_list, idx, len(dict_key_list_SHORTER), len(filenames_completed_list), len(dict_key_list), list_of_dates_in_df_list)\n",
    "\n",
    "            date_to_use = input('What date do you want to use')\n",
    "\n",
    "            substring = r\"^[\\-]{2}\"\n",
    "\n",
    "            if search(substring, date_to_use):\n",
    "                date_to_use = list_of_dates_in_df_list[0]\n",
    "            else:\n",
    "                date_to_use = date_to_use\n",
    "\n",
    "            # test that all rows have the correct filename\n",
    "            \n",
    "            #### AND ADD IN DIRECTOR NAME AS WELL ####\n",
    "            correct_row_bool_list = []\n",
    "\n",
    "            for i in range(0, len(index_to_correct_list)):\n",
    "\n",
    "                if df_check_dates.loc[index_to_correct_list[i], 'filename'] == filename_ and df_check_dates.loc[index_to_correct_list[i], 'director_name_round_four'] == dir_name:\n",
    "\n",
    "                    correct_row_bool_list.append(True)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    correct_row_bool_list.append(True)\n",
    "\n",
    "            correct_row_bool_list = list(set(correct_row_bool_list))\n",
    "\n",
    "            # if df_check_dates.loc[index_zero, 'filename'] == filename_:\n",
    "\n",
    "            if len(correct_row_bool_list) == 1 and correct_row_bool_list[0] == True:\n",
    "                \n",
    "                filenames_completed_list.append(elex)\n",
    "\n",
    "                for i in range(0, len(index_to_correct_list)):\n",
    "\n",
    "                    if doc_type_in_use == '3X':\n",
    "\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'appointment_date'] = date_to_use\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'event_date'] = date_to_use\n",
    "\n",
    "                        print('change made for', filename_, '; director:', dir_name, 'index:', index_to_correct_list[i], '; to:', date_to_use)\n",
    "\n",
    "                    elif doc_type_in_use == '3Z':\n",
    "\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'cessation_date'] = date_to_use\n",
    "                        df_check_dates.at[index_to_correct_list[i], 'event_date'] = date_to_use\n",
    "\n",
    "                        print('change made for', filename_, '; director:', dir_name, 'index:', index_to_correct_list[i], '; to:', date_to_use)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print('THERE IS AN ERROR!!!')\n",
    "            \n",
    "print('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_14JUL20_a.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: check that all coversheet dates are the same for the filename\n",
    "# TEST 2: check that all appointment dates and event dates are the same for the same filename, director and 3X\n",
    "# TEST 3: check that all cessation dates and event dates are the same for the same filename, director and 3Z\n",
    "\n",
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "number_of_rows_in_df = df_check_dates.shape[0]\n",
    "\n",
    "doc_type_list = ['3X', '3Y', '3Z']\n",
    "\n",
    "final_check_dict = {}\n",
    "\n",
    "# TEST 1: check that all coversheet dates are the same for the filename\n",
    "\n",
    "cover_sheet_date_test_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='first_test'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    coversheet_date_list = df_filtered_by_filename['cover_sheet_date']\n",
    "    \n",
    "    coversheet_date_list_tested = [ if_nan_return_True_otherwise_return_False(x) for x in coversheet_date_list ]\n",
    "    \n",
    "    coversheet_date_list_tested = list(set(coversheet_date_list_tested))\n",
    "    \n",
    "    if len(coversheet_date_list_tested) > 1:\n",
    "        \n",
    "        key = ele\n",
    "        \n",
    "        cover_sheet_date_test_dict.setdefault(key, [])\n",
    "        \n",
    "        cover_sheet_date_test_dict[key] = {'cover_sheet_dates': coversheet_date_list}\n",
    "    \n",
    "#         print(cover_sheet_date_test_dict[key])\n",
    "\n",
    "print('length of cover_sheet_date_test_dict is', len(cover_sheet_date_test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: check that all appointment dates and event dates are the same for the same filename, director and 3X\n",
    "\n",
    "appointment_date_3X_test_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='second_test'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    df_filtered_by_3X = df_filtered_by_filename[df_filtered_by_filename['document_type'] == '3X']\n",
    "    \n",
    "    dir_names_list = list(set(list(df_filtered_by_3X['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(dir_names_list):\n",
    "        \n",
    "        # filter for director name\n",
    "        \n",
    "        df_filtered_by_dir_name = df_filtered_by_3X[df_filtered_by_3X['director_name_round_four'] == elexx]\n",
    "        \n",
    "        # check for only one appointment date, event date and that they are both the same\n",
    "        \n",
    "        appt_date = list(set(list(df_filtered_by_dir_name['appointment_date'])))\n",
    "        event_date = list(set(list(df_filtered_by_dir_name['event_date'])))\n",
    "        \n",
    "        if (len(appt_date) > 1) or (len(event_date) > 1) or (appt_date != event_date):\n",
    "            \n",
    "            key = ele + r'_' + elexx + r'_' + '3X'\n",
    "            \n",
    "            appointment_date_3X_test_dict.setdefault(key, [])\n",
    "            \n",
    "            appointment_date_3X_test_dict[key] = {'filename': ele, 'director': elexx, 'document_type': '3X'}\n",
    "            \n",
    "print('length of appointment_date_3X_test_dict is', len(appointment_date_3X_test_dict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: check that all cessation dates and event dates are the same for the same filename, director and 3Z\n",
    "\n",
    "cessation_date_3Z_test_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='third_test'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    df_filtered_by_3Z = df_filtered_by_filename[df_filtered_by_filename['document_type'] == '3Z']\n",
    "    \n",
    "    dir_names_list = list(set(list(df_filtered_by_3Z['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(dir_names_list):\n",
    "        \n",
    "        # filter for director name\n",
    "        \n",
    "        df_filtered_by_dir_name = df_filtered_by_3Z[df_filtered_by_3Z['director_name_round_four'] == elexx]\n",
    "        \n",
    "        # check for only one cessation date, event date and that they are both the same\n",
    "        \n",
    "        cess_date = list(set(list(df_filtered_by_dir_name['cessation_date'])))\n",
    "        event_date = list(set(list(df_filtered_by_dir_name['event_date'])))\n",
    "        \n",
    "        if (len(cess_date) > 1) or (len(event_date) > 1) or (cess_date != event_date):\n",
    "            \n",
    "            key = ele + r'_' + elexx + r'_' + '3Z'\n",
    "            \n",
    "            cessation_date_3Z_test_dict.setdefault(key, [])\n",
    "            \n",
    "            cessation_date_3Z_test_dict[key] = {'filename': ele, 'director': elexx, 'document_type': '3Z'}   \n",
    "            \n",
    "print('length of cessation_date_3Z_test_dict is', len(cessation_date_3Z_test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of cover_sheet_date_test_dict is', len(cover_sheet_date_test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cessation_date_3Z_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ = '6093382.pdf'\n",
    "doc_type = '3Z'\n",
    "dir_name = 'Tony Harwood'\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['document_type'] == doc_type)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_correct = 37054\n",
    "\n",
    "corrected_date = '2012-04-18'\n",
    "\n",
    "if df_check_dates.loc[index_to_correct, 'director_name_round_four'] == dir_name:\n",
    "\n",
    "    if doc_type == '3X':\n",
    "        df_check_dates.at[index_to_correct, 'appointment_date'] = corrected_date\n",
    "    elif doc_type == '3Z':\n",
    "        df_check_dates.at[index_to_correct, 'cessation_date'] = corrected_date\n",
    "    \n",
    "    df_check_dates.at[index_to_correct, 'event_date'] = corrected_date\n",
    "    \n",
    "    print('correction made')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('THERE IS AN ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_16JUL20.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random integer values\n",
    "from random import seed\n",
    "from random import randint\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# generate some integers\n",
    "for _ in range(10):\n",
    "    index_ = randint(0, df_check_dates.shape[0])\n",
    "    \n",
    "    filename_ = df_check_dates.loc[index_, 'filename']\n",
    "    dir_name = df_check_dates.loc[index_, 'director_name_round_four']\n",
    "    doc_type = df_check_dates.loc[index_, 'document_type']\n",
    "    \n",
    "    if doc_type == '3X' or doc_type == '3Z':\n",
    "    \n",
    "        cover_sheet_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'cover_sheet_date'])\n",
    "        last_notice_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'last_notice_date'])\n",
    "        appointment_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'appointment_date'])\n",
    "        cessation_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'cessation_date'])\n",
    "        change_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'change_date'])\n",
    "        event_date = convert_df_cell_string_to_date_object_npnan_if_false(df_check_dates.loc[index_, 'event_date'])\n",
    "\n",
    "        openPDFFileFromFileName(filename_)\n",
    "        \n",
    "        result_string = 'filename:' + filename_ + \"\\n\" + 'director_name:' + dir_name + \"\\n\" + 'cover_sheet_date:' + cover_sheet_date #+ r\"\\n\" + 'last_notice_date:' + last_notice_date + r\"\\n\" + 'appointment_date:' + appointment_date + r\"\\n\" + 'change_date:' + change_date + r\"\\n\" + 'cessation_date:' + cessation_date + r\"\\n\" + 'event_date:' + event_date\n",
    "\n",
    "#         print(filename_, dir_name, doc_type, cover_sheet_date, last_notice_date, appointment_date, cessation_date, change_date, event_date)\n",
    "        print(result_string)\n",
    "        \n",
    "        wait = input('next one')\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record number check\n",
    "1. to see how many records each filename | director name | document type has\n",
    "2. eg Andrew Tsang has 40 records - this is 4 x 10 correct records - this error somehow arose during the document review stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "number_of_rows_in_df = df_check_dates.shape[0]\n",
    "\n",
    "doc_type_list = ['3X', '3Y', '3Z']\n",
    "\n",
    "record_number_check_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    dir_name_list = list(set(list(df_filtered_by_filename['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(dir_name_list):\n",
    "        \n",
    "        df_filtered_by_dir_name = df_filtered_by_filename[df_filtered_by_filename['director_name_round_four'] == elexx]\n",
    "        \n",
    "        for idxxx, elexxx in enumerate(doc_type_list):\n",
    "            \n",
    "            df_filtered_by_doc_type = df_filtered_by_dir_name[df_filtered_by_dir_name['document_type'] == elexxx]\n",
    "            \n",
    "            if df_filtered_by_doc_type.shape[0] > 0:\n",
    "                \n",
    "                key = ele + r'_' + elexx.replace(' ', '_') + r'_' + elexxx\n",
    "                \n",
    "                count_ = df_filtered_by_doc_type.shape[0]\n",
    "                \n",
    "                record_number_check_dict.setdefault(key, [])\n",
    "                \n",
    "                record_number_check_dict[key] = {'filename': ele, 'director_name': elexx, 'doc_type': elexxx, 'count': count_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_number_check_dict = record_number_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(record_number_check_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dictionary_of_key_values_into_df(dict_):\n",
    "\n",
    "    d = {'key': list(dict_.keys()),'values': list(dict_.values())}\n",
    "    df_dict = pd.DataFrame(data = d)\n",
    "\n",
    "    df_created = pd.concat([df_dict.drop(['values'], axis=1), df_dict['values'].apply(pd.Series)], axis=1)\n",
    "    \n",
    "    return df_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_check_no_3Y = df_count_check[~(df_count_check['doc_type'] == '3Y')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_check_no_3Y.sort_values([\"count\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_check_no_3Y[\n",
    "    (df_count_check_no_3Y['count'] % 4 == 0)\n",
    "    &\n",
    "    (df_count_check_no_3Y['doc_type'] == '3Z')\n",
    "].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_check_no_3Y_greater_than = df_count_check_no_3Y[df_count_check_no_3Y['count'] > 5].reset_index(drop=True)\n",
    "df_count_check_no_3Y_greater_than.shape[0]\n",
    "\n",
    "df_count_check_no_3Y_greater_than = df_count_check_no_3Y_greater_than.sort_values(by=['count'], ascending = False)\n",
    "\n",
    "df_count_check_no_3Y_greater_than['checked'] = int(0)\n",
    "df_count_check_no_3Y_greater_than['result'] = ''\n",
    "df_count_check_no_3Y_greater_than.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop to check number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_of_records = df_count_check_no_3Y_greater_than.shape[0]\n",
    "\n",
    "index_list_grand = df_count_check_no_3Y_greater_than.index.tolist()\n",
    "\n",
    "len_of_range_to_use = 42\n",
    "\n",
    "for a in range(0, len_of_range_to_use):\n",
    "    \n",
    "    i = index_list_grand[a]\n",
    "    \n",
    "    if df_count_check_no_3Y_greater_than.loc[i, 'checked'] == 0:\n",
    "        \n",
    "        filename_ = df_count_check_no_3Y_greater_than.loc[i, 'filename']\n",
    "        dir_name = df_count_check_no_3Y_greater_than.loc[i, 'director_name']\n",
    "        doc_type = df_count_check_no_3Y_greater_than.loc[i, 'doc_type']\n",
    "        count_ = df_count_check_no_3Y_greater_than.loc[i, 'count']\n",
    "        \n",
    "        # perform checks on indices of records\n",
    "        \n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == dir_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "        ]\n",
    "        \n",
    "        record_index_list = df_temp.index.tolist()\n",
    "        \n",
    "        record_index_list_sorted = sorted(record_index_list)\n",
    "        \n",
    "        index_len = len(record_index_list_sorted)\n",
    "        \n",
    "        start_int = record_index_list_sorted[0]\n",
    "        end_int = record_index_list_sorted[index_len - 1]\n",
    "        \n",
    "        if start_int + index_len - 1 == end_int:\n",
    "            \n",
    "            length_test_result = 'TEST OK'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            length_test_result = '__ LOOKS WRONG __'\n",
    "        \n",
    "        openPDFFileFromFileName(filename_)\n",
    "        \n",
    "        result_string_first = '___________' + '\\n' + str(a) + ' | ' + str(len_of_range_to_use)\n",
    "        \n",
    "        result_string_second = 'filename: ' + filename_ + '\\n' + 'director: ' + dir_name + '\\n' + 'doc_type: ' + doc_type + '\\n' + 'count: ' + str(count_) + '\\n' + 'record_index: ' + str(record_index_list)\n",
    "        \n",
    "        result_string_third = 'Length Test: ' + length_test_result\n",
    "        \n",
    "        result_string = result_string_first + '\\n' + result_string_second + '\\n' + result_string_third\n",
    "        \n",
    "        print(result_string)\n",
    "        \n",
    "        check_string = input('Ok?')\n",
    "        \n",
    "        if check_string == 'a':\n",
    "            \n",
    "            df_count_check_no_3Y_greater_than.at[i, 'result'] = 'OK'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_count_check_no_3Y_greater_than.at[i, 'result'] = 'ERROR'\n",
    "            \n",
    "        df_count_check_no_3Y_greater_than.at[i, 'checked'] = 1\n",
    "        \n",
    "        # SAVE DF IN LOOP\n",
    "\n",
    "        filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "        \n",
    "        filename_stem = r'df_record_count_check_16JUL20'\n",
    "\n",
    "        pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem + '.pickle'\n",
    "        \n",
    "        excel_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem + '.xlsx'\n",
    "\n",
    "        saveFileAsPickle(pickle_filepath_and_filename, df_count_check_no_3Y_greater_than)\n",
    "        \n",
    "        saveAsExcelFile(excel_filepath_and_filename, df_count_check_no_3Y_greater_than)\n",
    "        \n",
    "print('Done!')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_check_no_3Y_greater_than.loc[212]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save df_count_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_count_check_16JUL20_I.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_count_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "doc_type_list = ['3X', '3Z']\n",
    "\n",
    "record_number_check_string_test_dict = {}\n",
    "\n",
    "for idx, ele in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[df_check_dates['filename'] == ele]\n",
    "    \n",
    "    dir_name_list = list(set(list(df_filtered_by_filename['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(dir_name_list):\n",
    "        \n",
    "        df_filtered_by_dir_name = df_filtered_by_filename[df_filtered_by_filename['director_name_round_four'] == elexx]\n",
    "        \n",
    "        for idxxx, elexxx in enumerate(doc_type_list):\n",
    "            \n",
    "            df_filtered_by_doc_type = df_filtered_by_dir_name[df_filtered_by_dir_name['document_type'] == elexxx]\n",
    "            \n",
    "            concat_strings_list = []\n",
    "            \n",
    "            if df_filtered_by_doc_type.shape[0] > 1: # only need to test where there are two or more rows (multiple of two)\n",
    "            \n",
    "                # perform index list check\n",
    "\n",
    "                record_index_list = df_filtered_by_doc_type.index.tolist()\n",
    "\n",
    "                record_index_list_sorted = sorted(record_index_list)\n",
    "\n",
    "                index_len = len(record_index_list_sorted)\n",
    "                \n",
    "                for idxxxx, elexxxx in enumerate(record_index_list_sorted):\n",
    "                    \n",
    "                    string_1 = df_filtered_by_doc_type.loc[elexxxx, 'holder_name']\n",
    "                    string_2 = df_filtered_by_doc_type.loc[elexxxx, 'interest_type']\n",
    "                    string_3 = df_filtered_by_doc_type.loc[elexxxx, 'security_name_long']\n",
    "                    string_4 = df_filtered_by_doc_type.loc[elexxxx, 'security_name_short']\n",
    "                    string_5 = df_filtered_by_doc_type.loc[elexxxx, 'number']\n",
    "                    string_6 = df_filtered_by_doc_type.loc[elexxxx, 'exercise_price']\n",
    "                    string_7 = df_filtered_by_doc_type.loc[elexxxx, 'expiry_date']\n",
    "                    \n",
    "                    total_string = str(string_1) + str(string_2) + str(string_3) + str(string_4) + str(string_5) + str(string_6) + str(string_7)\n",
    "                    \n",
    "                    concat_strings_list.append(total_string)\n",
    "                \n",
    "                occurrences_list = collections.Counter(concat_strings_list)\n",
    "                \n",
    "                occurrences_list_unique_values = list(set(occurrences_list.values()))\n",
    "                \n",
    "#                 if ele == '5160892.pdf' and elexx == 'Andrew Tsang':\n",
    "                    \n",
    "#                     print(ele, elexx, elexxx, concat_strings_list, occurrences_list_unique_values)\n",
    "                \n",
    "                if len(occurrences_list_unique_values) == 1 and occurrences_list_unique_values[0] % 2 == 0:\n",
    "                    \n",
    "                    key = ele + r'_' + elexx.replace(' ', '_') + r'_' + elexxx\n",
    "\n",
    "                    record_number_check_string_test_dict.setdefault(key, [])\n",
    "\n",
    "                    record_number_check_string_test_dict[key] = {\n",
    "                        'filename': ele,\n",
    "                        'director_name': elexx,\n",
    "                        'doc_type': elexxx,\n",
    "                        'occurrences': occurrences_list_unique_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(record_number_check_string_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_possible_duplication = convert_dictionary_of_key_values_into_df(record_number_check_string_test_dict)\n",
    "\n",
    "df_with_possible_duplication['checked'] = 0\n",
    "df_with_possible_duplication['result'] = ''\n",
    "df_with_possible_duplication.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop to check for record count errors\n",
    "1. need to use the Excel | pickle file 'df_with_possible_duplication_16JUL20_USE_THIS.xlsx' to ensure double counting | rework does not occur - ignore those records already checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open existing processed df_with_possible_duplication_16JUL20_USE_THIS.xlsx file\n",
    "\n",
    "file_name_ = r'C:\\Users\\Paul\\backup_1Nov19\\adjata\\21_PICKLE_FILES\\df_with_possible_duplication_16JUL20_USE_THIS.xlsx'\n",
    "\n",
    "df_with_possible_duplication = openExcelFile(file_name_)\n",
    "\n",
    "df_with_possible_duplication.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df_with_possible_duplication.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df_with_possible_duplication.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_possible_duplication stats\n",
    "df_with_possible_duplication.shape[0], df_with_possible_duplication['checked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_of_records = df_with_possible_duplication.shape[0]\n",
    "\n",
    "index_list_grand = df_with_possible_duplication.index.tolist()\n",
    "\n",
    "len_of_range_to_use = 150\n",
    "\n",
    "for a in range(0, len_of_range_to_use):\n",
    "    \n",
    "    i = index_list_grand[a]\n",
    "    \n",
    "    if df_with_possible_duplication.loc[i, 'checked'] == 0:\n",
    "        \n",
    "        filename_ = df_with_possible_duplication.loc[i, 'filename']\n",
    "        dir_name = df_with_possible_duplication.loc[i, 'director_name']\n",
    "        doc_type = df_with_possible_duplication.loc[i, 'doc_type']\n",
    "        count_ = df_with_possible_duplication.loc[i, 'occurrences'][0]\n",
    "        \n",
    "        # perform checks on indices of records\n",
    "        \n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == dir_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "        ]\n",
    "        \n",
    "        record_index_list = df_temp.index.tolist()\n",
    "        \n",
    "        record_index_list_sorted = sorted(record_index_list)\n",
    "        \n",
    "        index_len = len(record_index_list_sorted)\n",
    "        \n",
    "        start_int = record_index_list_sorted[0]\n",
    "        end_int = record_index_list_sorted[index_len - 1]\n",
    "        \n",
    "        if start_int + index_len - 1 == end_int:\n",
    "            \n",
    "            length_test_result = 'TEST OK'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            length_test_result = '__ LOOKS WRONG __'\n",
    "        \n",
    "        openPDFFileFromFileName(filename_)\n",
    "        \n",
    "        result_string_first = '___________' + '\\n' + str(a) + ' | ' + str(len_of_range_to_use)\n",
    "        \n",
    "        result_string_second = 'filename: ' + filename_ + '\\n' + 'director: ' + dir_name + '\\n' + 'doc_type: ' + doc_type + '\\n' + 'record_index: ' + str(len(record_index_list))\n",
    "        \n",
    "#         result_string_third = 'Length Test: ' + length_test_result\n",
    "\n",
    "        result_string_third = 'OK: a, ERROR: e'\n",
    "        \n",
    "        result_string = result_string_first + '\\n' + result_string_second + '\\n' + result_string_third\n",
    "        \n",
    "        print(result_string)\n",
    "        \n",
    "        check_string = input('Ok?')\n",
    "        \n",
    "        if check_string.lower() == 'a':\n",
    "            \n",
    "            df_with_possible_duplication.at[i, 'result'] = 'OK'\n",
    "            \n",
    "        elif check_string.lower() == 'e':\n",
    "            \n",
    "            df_with_possible_duplication.at[i, 'result'] = 'ERROR'\n",
    "            \n",
    "        df_with_possible_duplication.at[i, 'checked'] = 1\n",
    "        \n",
    "        # SAVE DF IN LOOP\n",
    "\n",
    "        filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "        \n",
    "        filename_stem = r'df_with_possible_duplication_20JUL20'\n",
    "\n",
    "        pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem + '.pickle'\n",
    "        \n",
    "        excel_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem\n",
    "\n",
    "        saveFileAsPickle(pickle_filepath_and_filename, df_with_possible_duplication)\n",
    "        \n",
    "        saveAsExcelFile(excel_filepath_and_filename, df_with_possible_duplication)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use duplication df to determine the rows to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open existing processed df_with_possible_duplication_16JUL20_USE_THIS.xlsx file\n",
    "\n",
    "file_name_ = r'C:\\Users\\Paul\\backup_1Nov19\\adjata\\21_PICKLE_FILES\\df_with_possible_duplication_20JUL20.xlsx'\n",
    "\n",
    "df_with_possible_duplication = openExcelFile(file_name_)\n",
    "\n",
    "df_with_possible_duplication.drop('Column1', axis = 1, inplace = True)\n",
    "df_with_possible_duplication.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df_with_possible_duplication.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_possible_duplication['action'] = ''\n",
    "df_with_possible_duplication['action_checked'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_possible_duplication.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_to_correct_dict = {}\n",
    "\n",
    "index_list = df_with_possible_duplication.index.tolist()\n",
    "\n",
    "for idx, elex in enumerate(index_list):\n",
    "    \n",
    "    key = df_with_possible_duplication.loc[idx, 'key']\n",
    "    filename_ = df_with_possible_duplication.loc[idx, 'filename']\n",
    "    director_name = df_with_possible_duplication.loc[idx, 'director_name']\n",
    "    doc_type = df_with_possible_duplication.loc[idx, 'doc_type']\n",
    "    result_ = df_with_possible_duplication.loc[idx, 'result']\n",
    "    \n",
    "    if result_ == 'ERROR':\n",
    "    \n",
    "        records_to_correct_dict.setdefault(key, [])\n",
    "\n",
    "        records_to_correct_dict[key] = {\n",
    "            'filename': filename_,\n",
    "            'director_name': director_name,\n",
    "            'doc_type': doc_type,\n",
    "            'result': result_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records_to_check = df_with_possible_duplication['result'].value_counts()['ERROR']\n",
    "total_records_checked = df_with_possible_duplication['action_checked'].value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "\n",
    "for k, v in tqdm_notebook(records_to_correct_dict.items(), total=len(records_to_correct_dict), desc='main_loop'):\n",
    "    \n",
    "    key_row_index = df_with_possible_duplication.index[df_with_possible_duplication['key'] == k].tolist()\n",
    "    \n",
    "    print(key_row_index)\n",
    "    \n",
    "    if df_with_possible_duplication.loc[key_row_index[0], 'action_checked'] == 0:\n",
    "        \n",
    "        total_records_to_check = df_with_possible_duplication['result'].value_counts()['ERROR']\n",
    "        total_records_checked = df_with_possible_duplication['action_checked'].value_counts()[1]\n",
    "    \n",
    "        filename_ = v['filename']\n",
    "        director_name = v['director_name']\n",
    "        doc_type = v['doc_type']\n",
    "\n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == director_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "        ]\n",
    "\n",
    "        concat_strings_list = []\n",
    "\n",
    "        # perform index list check\n",
    "\n",
    "        record_index_list = df_temp.index.tolist()\n",
    "\n",
    "        record_index_list_sorted = sorted(record_index_list)\n",
    "\n",
    "        index_len = len(record_index_list_sorted)\n",
    "\n",
    "        unique_strings_separate_index_dict = {}\n",
    "\n",
    "        for idxxxx, elexxxx in enumerate(record_index_list_sorted):\n",
    "\n",
    "            string_1 = df_temp.loc[elexxxx, 'holder_name']\n",
    "            string_2 = df_temp.loc[elexxxx, 'interest_type']\n",
    "            string_3 = df_temp.loc[elexxxx, 'security_name_long']\n",
    "            string_4 = df_temp.loc[elexxxx, 'security_name_short']\n",
    "            string_5 = df_temp.loc[elexxxx, 'number']\n",
    "            string_6 = df_temp.loc[elexxxx, 'exercise_price']\n",
    "            string_7 = df_temp.loc[elexxxx, 'expiry_date']\n",
    "\n",
    "            total_string = str(string_1) + str(string_2) + str(string_3) + str(string_4) + str(string_5) + str(string_6) + str(string_7)\n",
    "\n",
    "            key_ = elexxxx\n",
    "            unique_strings_separate_index_dict.setdefault(key_, [])\n",
    "            unique_strings_separate_index_dict[key_] = {'unique_strg': total_string}\n",
    "\n",
    "            concat_strings_list.append(total_string)\n",
    "\n",
    "        occurrences_list = collections.Counter(concat_strings_list)\n",
    "\n",
    "        occurrences_list_unique_values = list(set(occurrences_list.values()))\n",
    "\n",
    "        unique_strings_all_indices_dict = {}\n",
    "\n",
    "        for kx in occurrences_list.keys():\n",
    "\n",
    "            key_ = kx\n",
    "            unique_strings_all_indices_dict.setdefault(key_, [])\n",
    "\n",
    "            indices_to_match = []\n",
    "\n",
    "            for kk, vv in unique_strings_separate_index_dict.items():\n",
    "\n",
    "                if vv['unique_strg'] == kx:\n",
    "\n",
    "                    indices_to_match.append(kk)\n",
    "\n",
    "            unique_strings_all_indices_dict[key_] = {'indices': indices_to_match}\n",
    "\n",
    "        # display the rows to be kept\n",
    "\n",
    "        list_to_keep = []\n",
    "        list_to_discard = []\n",
    "\n",
    "        for kk, vv in unique_strings_all_indices_dict.items():\n",
    "\n",
    "            temp_list = vv['indices']\n",
    "            temp_list_sorted = sorted(temp_list)\n",
    "\n",
    "            list_to_keep.append(temp_list_sorted[0])\n",
    "            list_to_discard.append(temp_list_sorted[1:])\n",
    "\n",
    "        string_to_display = ''\n",
    "        string_header_to_display = ''\n",
    "\n",
    "        for idd, eled in enumerate(list_to_keep):\n",
    "\n",
    "            string_1 = df_check_dates.loc[eled, 'holder_name']\n",
    "            string_2 = df_check_dates.loc[eled, 'interest_type']\n",
    "            string_3 = df_check_dates.loc[eled, 'security_name_long']\n",
    "            string_4 = df_check_dates.loc[eled, 'security_name_short']\n",
    "            string_5 = df_check_dates.loc[eled, 'number']\n",
    "            string_6 = df_check_dates.loc[eled, 'exercise_price']\n",
    "            string_7 = df_check_dates.loc[eled, 'expiry_date']\n",
    "\n",
    "            string_to_display = string_to_display + \"\\n\" + r'_________________________'\n",
    "            string_to_display = string_to_display + \"\\n\" + 'holder_name: ' + str(string_1)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'interest_type: ' + str(string_2)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'security_name_long: ' + str(string_3)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'security_name_short: ' + str(string_4)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'number: ' + str(string_5)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'exercise_price: ' + str(string_6)\n",
    "            string_to_display = string_to_display + \"\\n\" + 'expiry_date: ' + str(string_7)\n",
    "\n",
    "        openPDFFileFromFileName(filename_)\n",
    "\n",
    "        string_header_to_display = r'=========================' + \"\\n\" + filename_ + ' ' + director_name + ' ' + doc_type + ' ' + str(total_records_checked + 1) + r' | ' + str(total_records_to_check)\n",
    "\n",
    "        string_header_to_display = string_header_to_display + \"\\n\" + string_to_display\n",
    "\n",
    "        print(string_header_to_display)\n",
    "\n",
    "        response_ = input('Do you want to go ahead?')\n",
    "\n",
    "        DELETE_BOOL = False\n",
    "\n",
    "        if response_ == 'ok':\n",
    "\n",
    "            DELETE_BOOL = True\n",
    "\n",
    "        elif response_ == 'no':\n",
    "\n",
    "            DELETE_BOOL = False\n",
    "\n",
    "        elif response_ == 'xx':\n",
    "\n",
    "            DELETE_BOOL = False\n",
    "            xxxx\n",
    "\n",
    "        else:\n",
    "\n",
    "            DELETE_BOOL = False\n",
    "            xxxx\n",
    "\n",
    "        # delete rows if DELETE_BOOL = True\n",
    "\n",
    "        action_taken_dict = {True: 'ROWS DELETED', False: 'NO ROWS DELETED'}\n",
    "\n",
    "    #     key_row_index = df_with_possible_duplication.index[df_with_possible_duplication['key'] == k].tolist()\n",
    "\n",
    "        if DELETE_BOOL:\n",
    "\n",
    "            for ii, ee in enumerate(list_to_discard):\n",
    "\n",
    "                for iii, eee in enumerate(ee):\n",
    "\n",
    "                    print('row dropped:', eee)\n",
    "                    \n",
    "                    if df_check_dates.loc[eee, 'director_name_round_four'] == director_name:\n",
    "\n",
    "                        df_check_dates.drop([eee], inplace=True)\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        print('____________ ERROR ____________')\n",
    "                        XXXX\n",
    "\n",
    "            df_with_possible_duplication.loc[key_row_index, 'action'] = action_taken_dict[DELETE_BOOL]\n",
    "            df_with_possible_duplication.loc[key_row_index, 'action_checked'] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            df_with_possible_duplication.loc[key_row_index, 'action'] = action_taken_dict[DELETE_BOOL]\n",
    "            df_with_possible_duplication.loc[key_row_index, 'action_checked'] = 1\n",
    "\n",
    "        # create dictionary of actions taken on rows\n",
    "\n",
    "        keya = k\n",
    "\n",
    "        action_dict.setdefault(keya, [])\n",
    "\n",
    "        kept_dict = {}\n",
    "\n",
    "        for idd, eled in enumerate(list_to_keep):\n",
    "\n",
    "            key_ = eled\n",
    "            kept_dict.setdefault(key_, [])\n",
    "\n",
    "            kept_dict[key_] = {'index': key_, 'string_kept': unique_strings_separate_index_dict[key_], 'ACTION': action_taken_dict[DELETE_BOOL]}\n",
    "\n",
    "        deleted_dict = {}\n",
    "\n",
    "        for idd, eled in enumerate(list_to_discard):\n",
    "\n",
    "            for iddd, eledd in enumerate(eled):\n",
    "\n",
    "                key_ = eledd\n",
    "                deleted_dict.setdefault(key_, [])\n",
    "\n",
    "                deleted_dict[key_] = {'index': key_, 'string_deleted': unique_strings_separate_index_dict[key_], 'ACTION': action_taken_dict[DELETE_BOOL]}    \n",
    "\n",
    "        action_dict[keya] = {'kept': kept_dict, 'deleted': deleted_dict}\n",
    "\n",
    "        # save dictionary\n",
    "\n",
    "        df_of_action_items = convert_dictionary_of_key_values_into_df(action_dict)\n",
    "\n",
    "        # SAVE DF IN LOOP\n",
    "\n",
    "        filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "        filename_stem = r'df_of_action_items_20JUL20'\n",
    "\n",
    "        # save detailed description of action taken df\n",
    "\n",
    "        pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem + '.pickle'\n",
    "\n",
    "        excel_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem\n",
    "\n",
    "        saveFileAsPickle(pickle_filepath_and_filename, df_of_action_items)\n",
    "\n",
    "        saveAsExcelFile(excel_filepath_and_filename, df_of_action_items)   \n",
    "\n",
    "        # save duplication file df\n",
    "\n",
    "        filename_stem = r'df_with_possible_duplication_with_action_taken_20JUL20'\n",
    "\n",
    "        pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem + '.pickle'\n",
    "\n",
    "        excel_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + filename_stem\n",
    "\n",
    "        saveFileAsPickle(pickle_filepath_and_filename, df_with_possible_duplication)\n",
    "\n",
    "        saveAsExcelFile(excel_filepath_and_filename, df_with_possible_duplication)    \n",
    "\n",
    "        print(filename_, director_name, doc_type, list_to_keep, list_to_discard)\n",
    "    \n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_20JUL20_DEDUPLICATED.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensure that when the interest_type (Direct | Indirect) is Direct, the holder_name is the same as director_name_round_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename_ = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "doc_type_list = ['3X', '3Z']\n",
    "interest_type_list = ['Direct', 'Indirect']\n",
    "\n",
    "interest_type_check_dict = {}\n",
    "\n",
    "for idx, elex in tqdm_notebook(enumerate(filenames_), total=len(filenames_), desc='main_loop'):\n",
    "    \n",
    "    df_filtered_by_filename = df_check_dates[\n",
    "        (df_check_dates['filename'] == elex)\n",
    "    ]\n",
    "    \n",
    "    dir_name_list = list(set(list(df_filtered_by_filename['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(dir_name_list):\n",
    "        \n",
    "        df_filtered_by_director = df_filtered_by_filename[\n",
    "            (df_filtered_by_filename['director_name_round_four'] == elexx)\n",
    "        ]\n",
    "        \n",
    "        for ii, ee in enumerate(doc_type_list):\n",
    "        \n",
    "            df_filtered_by_doc_type = df_filtered_by_director[\n",
    "                (df_filtered_by_director['document_type'] == ee)\n",
    "            ]\n",
    "            \n",
    "            if df_filtered_by_doc_type.shape[0] > 0:\n",
    "                \n",
    "                spec_index_list = df_filtered_by_doc_type.index.tolist()\n",
    "                \n",
    "                for i in range(0, len(spec_index_list)):\n",
    "                    \n",
    "                    index_ = spec_index_list[i]\n",
    "                    \n",
    "                    try:\n",
    "            \n",
    "                        number_int = float(df_filtered_by_doc_type.loc[index_, 'number'])\n",
    "\n",
    "    #                     print(number_int, type(number_int))\n",
    "\n",
    "                        if if_nan_return_True_otherwise_return_False(number_int):\n",
    "\n",
    "                            number_int = 0\n",
    "\n",
    "    #                     print(number_int)\n",
    "\n",
    "                        if number_int > 0:\n",
    "\n",
    "                            interest_type_ = df_filtered_by_doc_type.loc[index_, 'interest_type']\n",
    "                            holder_name_ = df_filtered_by_doc_type.loc[index_, 'holder_name']\n",
    "\n",
    "#                             if interest_type_ == 'Direct' and (holder_name_ != elexx): # TEST 1\n",
    "                            if interest_type_ == 'Indirect' and (holder_name_ == elexx): # TEST 2\n",
    "\n",
    "                                key = elex + '_' + elexx.replace(' ', '_') + '_' + ee + '_' + str(index_)\n",
    "                                interest_type_check_dict.setdefault(key, [])\n",
    "\n",
    "                                interest_type_check_dict[key] = {\n",
    "                                    'filename': elex,\n",
    "                                    'dir_name': elexx,\n",
    "                                    'doc_type': ee,\n",
    "                                    'interest_type': interest_type_,\n",
    "                                    'holder_name': holder_name_,\n",
    "                                    'number': number_int\n",
    "                                }\n",
    "\n",
    "                                print(interest_type_check_dict[key])\n",
    "                                \n",
    "                    except:\n",
    "                        \n",
    "                        key = elex + '_' + elexx.replace(' ', '_') + '_' + ee\n",
    "                        interest_type_check_dict.setdefault(key, [])\n",
    "\n",
    "                        interest_type_check_dict[key] = {\n",
    "                            'filename': elex,\n",
    "                            'dir_name': elexx,\n",
    "                            'doc_type': ee,\n",
    "                            'interest_type': 'ERROR',\n",
    "                            'holder_name': 'ERROR',\n",
    "                            'number': number_int\n",
    "                        }\n",
    "\n",
    "                        print(interest_type_check_dict[key])\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_type_check_ACTION_dict = {}\n",
    "\n",
    "for k, v in interest_type_check_dict.items():\n",
    "    \n",
    "    filename_ = v['filename']\n",
    "    dir_name = v['dir_name']\n",
    "    doc_type = v['doc_type']\n",
    "    interest_type = v['interest_type']\n",
    "    holder_name = v['holder_name']\n",
    "    number_ = v['number']\n",
    "    \n",
    "    if dir_name.lower() == holder_name.lower() and interest_type == 'Indirect':\n",
    "        \n",
    "        action_ = 'CHANGE_TO_INDIRECT'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        action_ = ''\n",
    "        \n",
    "    key = k\n",
    "    interest_type_check_ACTION_dict.setdefault(key, [])\n",
    "    \n",
    "    interest_type_check_ACTION_dict[key] = {\n",
    "        'filename': filename_,\n",
    "        'dir_name': dir_name,\n",
    "        'doc_type': doc_type,\n",
    "        'interest_type': interest_type,\n",
    "        'holder_name': holder_name,\n",
    "        'number': number_,\n",
    "        'ACTION': action_\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_type_check_ACTION_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_type_check_MORE_ACTION_dict = {}\n",
    "\n",
    "counter_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k, v in interest_type_check_ACTION_dict.items():\n",
    "    \n",
    "    if v['interest_type'] != 'ERROR' and v['ACTION'] != 'CHANGE_CASE':\n",
    "        \n",
    "        counter_ = counter_ + 1\n",
    "        \n",
    "        filename_ = v['filename']\n",
    "        dir_name = v['dir_name']\n",
    "        doc_type = v['doc_type']\n",
    "        interest_type = v['interest_type']\n",
    "        holder_name = v['holder_name']\n",
    "        number_ = v['number']\n",
    "        \n",
    "        string_to_display = ''\n",
    "\n",
    "        string_to_display = string_to_display + \"\\n\" + r'_________________________'\n",
    "        string_to_display = string_to_display + \"\\n\" + 'filename: ' + str(filename_)\n",
    "        string_to_display = string_to_display + \"\\n\" + 'director_name: ' + str(dir_name)\n",
    "        string_to_display = string_to_display + \"\\n\" + 'document_type: ' + str(doc_type)\n",
    "        string_to_display = string_to_display + \"\\n\" + 'interest_type: ' + str(interest_type)\n",
    "        string_to_display = string_to_display + \"\\n\" + 'holder_name: ' + str(holder_name)\n",
    "        string_to_display = string_to_display + \"\\n\" + 'number: ' + str(number_)\n",
    "            \n",
    "\n",
    "        openPDFFileFromFileName(filename_)\n",
    "\n",
    "        string_header_to_display = r'=========================' + ' ' + str(counter_) + ' | ' + str(len(interest_type_check_ACTION_dict))\n",
    "        \n",
    "        instructions_ = \"cc: CHANGE_OVER_NAME | dd: CHANGE_TO_DIRECT | ii: CHANGE_TO_INDIRECT\"\n",
    "\n",
    "        string_header_to_display = string_header_to_display + \"\\n\" + string_to_display + \"\\n\" + instructions_\n",
    "\n",
    "        print(string_header_to_display)\n",
    "\n",
    "        response_ = input('What action?')\n",
    "        \n",
    "        indirect_holder_name = ''\n",
    "\n",
    "        if response_ == 'cc':\n",
    "            \n",
    "            ACTION_ = 'CHANGE_OVER_NAME'\n",
    "            \n",
    "        elif response_ == 'dd':\n",
    "            \n",
    "            ACTION_ = 'CHANGE_TO_DIRECT'\n",
    "            \n",
    "        elif response_ == 'ii':\n",
    "            \n",
    "            ACTION_ = 'CHANGE_TO_INDIRECT'\n",
    "            \n",
    "            indirect_holder_name = input('What indirect holder name?')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            xxxxx\n",
    "            \n",
    "        key = k\n",
    "        interest_type_check_MORE_ACTION_dict.setdefault(key, [])\n",
    "        \n",
    "        interest_type_check_MORE_ACTION_dict[key] = {\n",
    "            'filename': filename_,\n",
    "            'dir_name': dir_name,\n",
    "            'doc_type': doc_type,\n",
    "            'interest_type': interest_type,\n",
    "            'holder_name': holder_name,\n",
    "            'number': number_,\n",
    "            'ACTION': ACTION_,\n",
    "            'indirect holder_name': indirect_holder_name\n",
    "        }\n",
    "        \n",
    "print('done!')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_type_check_MORE_ACTION_dict['903276.pdf_Tonianne_Dwyer_3Z_45737']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_type_check_MORE_ACTION_RESULT_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in interest_type_check_MORE_ACTION_dict.items():\n",
    "    \n",
    "    filename_ = v['filename']\n",
    "    dir_name = v['dir_name']\n",
    "    doc_type = v['doc_type']\n",
    "    interest_type = v['interest_type']\n",
    "    holder_name = v['holder_name']\n",
    "    number_ = v['number']\n",
    "    action_ = v['ACTION']\n",
    "    indirect_holder_name = v['indirect holder_name']\n",
    "    \n",
    "    RESULT_BOOL = ''\n",
    "    \n",
    "    if action_ == 'CHANGE_OVER_NAME':\n",
    "        \n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == dir_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "            &\n",
    "            (df_check_dates['interest_type'] == interest_type)\n",
    "            &\n",
    "            (df_check_dates['holder_name'] == holder_name)\n",
    "            &\n",
    "            (df_check_dates['number'] == number_)\n",
    "        ]\n",
    "        \n",
    "#         print(df_temp.shape[0])\n",
    "        \n",
    "        index_list = df_temp.index.tolist()\n",
    "\n",
    "        for idx, elex in enumerate(index_list):\n",
    "            \n",
    "            if df_check_dates.loc[elex, 'director_name_round_four'] == dir_name and df_check_dates.loc[elex, 'holder_name'] == holder_name:\n",
    "                \n",
    "                df_check_dates.at[elex, 'holder_name'] = dir_name\n",
    "                \n",
    "                string_to_display = 'holder_name: ' + df_check_dates.loc[elex, 'holder_name'] + \"\\n\" + 'dir_name: ' + df_check_dates.loc[elex, 'director_name_round_four']\n",
    "                \n",
    "                print(string_to_display)\n",
    "            \n",
    "                RESULT_BOOL = 'CHANGE_MADE'\n",
    "                \n",
    "    elif action_ == 'CHANGE_TO_INDIRECT':\n",
    "        \n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == dir_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "            &\n",
    "            (df_check_dates['interest_type'] == interest_type)\n",
    "            &\n",
    "            (df_check_dates['holder_name'] == holder_name)\n",
    "            &\n",
    "            (df_check_dates['number'] == number_)\n",
    "        ]\n",
    "        \n",
    "        index_list = df_temp.index.tolist()\n",
    "\n",
    "        for idx, elex in enumerate(index_list):\n",
    "            \n",
    "            if df_check_dates.loc[elex, 'director_name_round_four'] == dir_name and df_check_dates.loc[elex, 'holder_name'] == holder_name and df_check_dates.loc[elex, 'interest_type'] == interest_type:\n",
    "                \n",
    "                df_check_dates.at[elex, 'interest_type'] = 'Indirect'\n",
    "                df_check_dates.at[elex, 'holder_name'] = indirect_holder_name\n",
    "                \n",
    "                string_to_display = 'holder_name: ' + df_check_dates.loc[elex, 'holder_name'] + \"\\n\" + 'dir_name: ' + df_check_dates.loc[elex, 'director_name_round_four']\n",
    "                \n",
    "                print('Indirect', string_to_display, indirect_holder_name)\n",
    "            \n",
    "                RESULT_BOOL = 'CHANGE_MADE'\n",
    "                \n",
    "    elif action_ == 'CHANGE_TO_DIRECT':\n",
    "        \n",
    "        df_temp = df_check_dates[\n",
    "            (df_check_dates['filename'] == filename_)\n",
    "            &\n",
    "            (df_check_dates['director_name_round_four'] == dir_name)\n",
    "            &\n",
    "            (df_check_dates['document_type'] == doc_type)\n",
    "            &\n",
    "            (df_check_dates['interest_type'] == interest_type)\n",
    "            &\n",
    "            (df_check_dates['holder_name'] == holder_name)\n",
    "            &\n",
    "            (df_check_dates['number'] == number_)\n",
    "        ]\n",
    "        \n",
    "        index_list = df_temp.index.tolist()\n",
    "\n",
    "        for idx, elex in enumerate(index_list):\n",
    "            \n",
    "            if df_check_dates.loc[elex, 'director_name_round_four'] == dir_name and df_check_dates.loc[elex, 'holder_name'] == holder_name and df_check_dates.loc[elex, 'interest_type'] == interest_type:\n",
    "                \n",
    "                df_check_dates.at[elex, 'interest_type'] = 'Direct'\n",
    "                \n",
    "                string_to_display = 'holder_name: ' + df_check_dates.loc[elex, 'holder_name'] + \"\\n\" + 'dir_name: ' + df_check_dates.loc[elex, 'director_name_round_four']\n",
    "                \n",
    "                print(string_to_display)\n",
    "            \n",
    "                RESULT_BOOL = 'CHANGE_MADE'\n",
    "        \n",
    "            \n",
    "    key = k\n",
    "    interest_type_check_MORE_ACTION_RESULT_dict.setdefault(key, [])\n",
    "\n",
    "    interest_type_check_MORE_ACTION_RESULT_dict[key] = {\n",
    "        'filename': filename_,\n",
    "        'dir_name': dir_name,\n",
    "        'doc_type': doc_type,\n",
    "        'interest_type': interest_type,\n",
    "        'holder_name': holder_name,\n",
    "        'number': number_,\n",
    "        'ACTION': action_,\n",
    "        'RESULT': RESULT_BOOL\n",
    "    }\n",
    "        \n",
    "#         df_check_dates.at[elex, 'holder_name'] == dir_name\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: 7355242.pdf\n",
    "# director_name: Keith Raymond Smith\n",
    "# document_type: 3X\n",
    "# interest_type: Indirect\n",
    "# holder_name: Keith Raymond Smith\n",
    "# number: 80.0\n",
    "\n",
    "filename_ = '7355242.pdf'\n",
    "dir_name = 'Keith Raymond Smith'\n",
    "doc_type = '3X'\n",
    "# interest_type = 'Direct'\n",
    "number_ = 80.0\n",
    "\n",
    "df_temp = df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "    &\n",
    "    (df_check_dates['document_type'] == doc_type)\n",
    "#     &\n",
    "#     (df_check_dates['interest_type'] == interest_type)\n",
    "    &\n",
    "    (df_check_dates['number'] == number_)\n",
    "]\n",
    "\n",
    "print(df_temp['holder_name'], df_temp['interest_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_24JUL20_II.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in interest_type_check_MORE_ACTION_RESULT_dict.items():\n",
    "    \n",
    "    if v['ACTION'] == 'CHANGE_OVER_NAME' and v['RESULT'] == 'CHANGE_MADE':\n",
    "        \n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01881493.pdf Mark Hamish Lochtenberg 3Z\n",
    "\n",
    "# filename_ = '656649.pdf'\n",
    "dir_name = 'Simon David Yeo'\n",
    "# dir_name = 'Russell Peter Kane'\n",
    "# doc_type = '3Z'\n",
    "\n",
    "df_test = df_check_dates[\n",
    "#     (df_check_dates['filename'] == filename_)\n",
    "#     &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "#     (df_check_dates['director_name_round_four'] == dir_name)\n",
    "#     &\n",
    "#     (df_check_dates['document_type'] == doc_type)\n",
    "]\n",
    "\n",
    "filename_list = list(set(list(df_test['filename'])))\n",
    "dir_name_list = dir_name_old_list = list(set(list(df_test['director_name_round_four'])))\n",
    "dir_name_old_list = list(set(list(df_test['director_name_round_four_old'])))\n",
    "holder_name_list = list(set(list(df_test['holder_name'])))\n",
    "index_list = df_test.index.tolist()\n",
    "doc_type_list = list(set(list(df_test['document_type'])))\n",
    "\n",
    "print(filename_list, dir_name_old_list, dir_name_list, holder_name_list, index_list, doc_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(index_list)):\n",
    "    \n",
    "    index_ = index_list[i]\n",
    "    \n",
    "#     df_check_dates.at[index_, 'director_name_round_four'] = dir_name_old_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simon_david_yeo_dict = {}\n",
    "\n",
    "for idx, elex in enumerate(filename_list):\n",
    "    \n",
    "    df_test = df_check_dates[(df_check_dates['filename'] == elex)]\n",
    "    \n",
    "    doc_type_list = list(set(list(df_test['document_type'])))\n",
    "    \n",
    "    if '3X' in doc_type_list or '3Z' in doc_type_list:\n",
    "    \n",
    "        openPDFFileFromFileName(elex)\n",
    "\n",
    "        wait_ = input('next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'656649.pdf': 'Simon David Yeo'\n",
    "'6566502.pdf': 'Russell Peter Kane'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openPDFFileFromFileName(filename_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes from merging:\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. ('749904.pdf', '3X', 'Jason William Peterson')\n",
    "# Appears in df_SINGLE_3X_.pickle but not in df_combined_adjusted_for_consolidated_ENTITY_NAMES_and_MANUAL_CHANGES_DIRECTOR_NAME_REVIEW_6JUL20.pickle\n",
    "# This record must have manually been created by me in the Excel file when processing the review of 3X | 3Z files\n",
    "# ACTION:\n",
    "# Create Excel file out of source pickle file - add the records for ('749904.pdf', '3X', 'Jason William Peterson')\n",
    "# and convert back to pickle file\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. ('838927.pdf', '3X', 'Allan Preston Beasley')\n",
    "# Appears in df_SINGLE_3X_.pickle but not in df_combined_adjusted_for_consolidated_ENTITY_NAMES_and_MANUAL_CHANGES_DIRECTOR_NAME_REVIEW_6JUL20.pickle\n",
    "# This record must have manually been created by me in the Excel file when processing the review of 3X | 3Z files\n",
    "# ACTION:\n",
    "# Create Excel file out of source pickle file - add the records for ('838927.pdf', '3X', 'Allan Preston Beasley')\n",
    "# and convert back to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for null 'number' instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = list(set(list(df_check_dates['filename'])))\n",
    "\n",
    "df_check_dates_filtered = df_check_dates[\n",
    "    ~(df_check_dates['document_type'] == '3Y')\n",
    "]\n",
    "\n",
    "doc_type_list = ['3X', '3Z']\n",
    "\n",
    "check_number_for_null_dict = {}\n",
    "\n",
    "check_type = 'Indirect_check_for_null_number'\n",
    "\n",
    "for idx, elex in tqdm_notebook(enumerate(filename_list), total=len(filename_list), desc='main_loop'):\n",
    "    \n",
    "    # filter by filename\n",
    "    df_temp_I = df_check_dates_filtered[df_check_dates_filtered['filename'] == elex]\n",
    "    \n",
    "    director_name_list = list(set(list(df_temp_I['director_name_round_four'])))\n",
    "    \n",
    "    for idxx, elexx in enumerate(director_name_list):\n",
    "        \n",
    "        # filter by director name\n",
    "        df_temp_II = df_temp_I[df_temp_I['director_name_round_four'] == elexx]\n",
    "        \n",
    "        for ii, ee in enumerate(doc_type_list):\n",
    "            \n",
    "            # filter by document type\n",
    "            df_temp_III = df_temp_II[df_temp_II['document_type'] == ee]\n",
    "        \n",
    "            index_error_list = [] \n",
    "            \n",
    "            index_list = df_temp_III.index.tolist()\n",
    "            \n",
    "            for aa, bb in enumerate(index_list):\n",
    "            \n",
    "                interest_type = df_check_dates.loc[bb, 'interest_type']\n",
    "                holder_name = df_check_dates.loc[bb, 'holder_name']\n",
    "                security_name_long = df_check_dates.loc[bb, 'security_name_long']\n",
    "                security_name_short = df_check_dates.loc[bb, 'security_name_short']\n",
    "                \n",
    "                number_ = df_check_dates.loc[bb, 'number']\n",
    "                exercise_price = df_check_dates.loc[bb, 'exercise_price']\n",
    "                \n",
    "                expiry_date = df_check_dates.loc[bb, 'expiry_date']\n",
    "                \n",
    "                if expiry_date == 0:\n",
    "                    \n",
    "                    expiry_date = 'NaN'\n",
    "                \n",
    "                try:\n",
    "                    expiry_date = convert_df_cell_string_to_date_object(df_check_dates.loc[bb, 'expiry_date'])\n",
    "\n",
    "#                     if interest_type == 'Indirect' and (len(str(holder_name)) > 3 and holder_name != elexx) and (number_ < 1 and ((exercise_price > 0 and len(str(expiry_date)) < 3) or (exercise_price == 0 and len(str(expiry_date)) > 3))):\n",
    "                    if interest_type == 'Direct' and (len(str(holder_name)) > 3 and holder_name != elexx) and (number_ < 1):\n",
    "#                     if interest_type == 'Direct' and (len(str(holder_name)) > 3 and holder_name != elexx) and (number_ < 1 and ((exercise_price > 0 and len(str(expiry_date)) < 3) or (exercise_price == 0 and len(str(expiry_date)) > 3))):\n",
    "\n",
    "                        key = str(elex) + r'_' + str(elexx.replace(' ', '_')) + '_' + ee + '_' + str(bb)\n",
    "\n",
    "                        check_number_for_null_dict.setdefault(key, [])\n",
    "\n",
    "                        check_number_for_null_dict[key] = {\n",
    "                            'check_type': check_type,\n",
    "                            'filename': elex,\n",
    "                            'director': elexx,\n",
    "                            'doc_type': ee,\n",
    "                            'index': bb,\n",
    "                            'interest_type': interest_type,\n",
    "                            'holder': holder_name,\n",
    "                            'security_name_long': security_name_long,\n",
    "                            'security_name_short': security_name_short,\n",
    "                            'number': number_,\n",
    "                            'exercise_price': exercise_price,\n",
    "                            'expiry_date': expiry_date,\n",
    "                            'TEST': 'OK'\n",
    "                        }\n",
    "                        \n",
    "                except:\n",
    "                    \n",
    "                    key = str(elex) + r'_' + str(elexx.replace(' ', '_')) + '_' + ee + '_' + str(bb)\n",
    "                    \n",
    "                    check_number_for_null_dict.setdefault(key, [])\n",
    "\n",
    "                    check_number_for_null_dict[key] = {\n",
    "                        'check_type': check_type,\n",
    "                        'filename': elex,\n",
    "                        'director': elexx,\n",
    "                        'doc_type': ee,\n",
    "                        'index': bb,\n",
    "                        'interest_type': interest_type,\n",
    "                        'holder': holder_name,\n",
    "                        'security_name_long': security_name_long,\n",
    "                        'security_name_short': security_name_short,\n",
    "                        'number': number_,\n",
    "                        'exercise_price': exercise_price,\n",
    "                        'expiry_date': 'ERROR',\n",
    "                        'TEST': 'DATE_ERROR'\n",
    "                    }\n",
    "                \n",
    "                # *******************************\n",
    "\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_number_for_null_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_to_check = []\n",
    "\n",
    "for k, v in check_number_for_null_dict.items():\n",
    "    \n",
    "    index_list_to_check.append(v['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ = index_list_to_check[2]   \n",
    "\n",
    "filename_ = df_check_dates.loc[index_, 'filename']\n",
    "dir_name = df_check_dates.loc[index_, 'director_name_round_four']\n",
    "holder_ = df_check_dates.loc[index_, 'holder_name']\n",
    "number_ = df_check_dates.loc[index_, 'number']\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "print(index_, '|', filename_, '|', dir_name, '|', holder_, '|', number_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_check_dates.at[index_, 'number'] = 1898078\n",
    "df_check_dates.at[index_, 'holder_name'] = 'Simon Lucien Tregoning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_check_dates.pickle file\n",
    "\n",
    "filepath_filename_tuple = os.path.split(os.path.abspath(df_GRAND_SOURCE))\n",
    "\n",
    "pickle_filepath_and_filename = filepath_filename_tuple[0] + u'\\\\' + 'df_GRAND_3X_3Y_3Z_file_MERGED_NAMES_24JUL20_VI.pickle'\n",
    "\n",
    "saveFileAsPickle(pickle_filepath_and_filename, df_check_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in check_number_for_null_dict.items():\n",
    "    \n",
    "    if v['TEST'] == 'DATE_ERROR':\n",
    "        \n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 432752.pdf_Christopher_Quirk_3Y_273833\n",
    "\n",
    "filename_ = '432752.pdf'\n",
    "dir_name = 'Christopher Quirk'\n",
    "document_type = '3Y'\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "    &\n",
    "    (df_check_dates['document_type'] == document_type)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_errors_dictionary = r'C:\\Users\\Paul\\backup_1Nov19\\adjata\\21_PICKLE_FILES\\check_3X_3Z_dates_filename_and_coversheet_dict_13JUL20.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_error_dict = open_pickle_file_as_df(possible_errors_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in possible_error_dict.items():\n",
    "    \n",
    "    if v['doc_type'] == '3X':\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ = '607332.pdf'\n",
    "dir_name = 'James Lewis Michael Malone'\n",
    "\n",
    "filename_ = '01792355.pdf'\n",
    "dir_name = 'Ashish Patel'\n",
    "\n",
    "filename_ = '5160892.pdf'\n",
    "dir_name = 'Andrew Tsang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 E:\\announcements_COPIED\\2015\\10\\06\\883218.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Peter Charles Constable & R J Constable ATF the Joint Account A/C'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'filename': '883218.pdf',\n",
    "#   'index': 525,\n",
    "#   'holder_name': 'Peter Charles Constable & R J Constable ATF the Joint Account A/C'},\n",
    "\n",
    "filename_ = '883218.pdf'\n",
    "# dir_name = 'Vasile Timis'\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "#     &\n",
    "#     (df_check_dates['director_name_round_four'] == dir_name)\n",
    "]\n",
    "\n",
    "df_check_dates.loc[525, 'holder_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.loc[4829, 'holder_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.at[4829, 'holder_name'] = 'Langham Investments Pty Ltd ATF the Langham Family Trust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = df_check_dates.index.tolist()\n",
    "\n",
    "check_for_unwanted_int_dict = {}\n",
    "\n",
    "column_to_test = 'director_name_round_four'\n",
    "\n",
    "for idx, elex in tqdm_notebook(enumerate(index_list), total=len(index_list), desc='main_loop'):\n",
    "    \n",
    "    test_element = df_check_dates.loc[elex, column_to_test]\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        test_this = int(test_element)\n",
    "        \n",
    "        key = df_check_dates.loc[elex, 'filename'] + '_' + str(elex)\n",
    "        check_for_unwanted_int_dict.setdefault(key, [])\n",
    "        \n",
    "        check_for_unwanted_int_dict[key] = {\n",
    "            'column_to_test': column_to_test,\n",
    "            'filename': df_check_dates.loc[elex, 'filename'],\n",
    "            'index': elex,\n",
    "            'holder_name': test_element\n",
    "        }\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8656f06a83fd4b25857d53dbd7ca90a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='main_loop', max=276126, style=ProgressStyle(description_width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_list = df_check_dates.index.tolist()\n",
    "\n",
    "check_for_unwanted_x_dict = {}\n",
    "\n",
    "column_to_test = 'change_nature'\n",
    "\n",
    "for idx, elex in tqdm_notebook(enumerate(index_list), total=len(index_list), desc='main_loop'):\n",
    "    \n",
    "    test_element = df_check_dates.loc[elex, column_to_test]\n",
    "    dir_name = df_check_dates.loc[elex, 'director_name_round_four']\n",
    "    doc_type = df_check_dates.loc[elex, 'document_type']\n",
    "    \n",
    "    if isinstance(test_element, str) and 'xx' in test_element.lower() and test_element != dir_name and doc_type != '3Y':\n",
    "        \n",
    "        key = df_check_dates.loc[elex, 'filename'] + '_' + str(elex)\n",
    "        check_for_unwanted_x_dict.setdefault(key, [])\n",
    "\n",
    "        check_for_unwanted_x_dict[key] = {\n",
    "            'column_to_test': column_to_test,\n",
    "            'filename': df_check_dates.loc[elex, 'filename'],\n",
    "            'index': elex,\n",
    "            column_to_test: test_element\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_for_unwanted_x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'504325.pdf_9184': {'column_to_test': 'change_nature',\n",
       "  'filename': '504325.pdf',\n",
       "  'index': 9184,\n",
       "  'change_nature': 'Held in Allco Finance Group lxx'},\n",
       " '502012.pdf_42863': {'column_to_test': 'change_nature',\n",
       "  'filename': '502012.pdf',\n",
       "  'index': 42863,\n",
       "  'change_nature': 'Held in Allco Hybrid ixx Trust'},\n",
       " '301844.pdf_50204': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50204,\n",
       "  'change_nature': 'held in singapore telecommunications lxx'},\n",
       " '301844.pdf_50205': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50205,\n",
       "  'change_nature': 'held in singapore computer systems lxx'},\n",
       " '301844.pdf_50206': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50206,\n",
       "  'change_nature': 'held in singapore technologies engineering lxx'},\n",
       " '301844.pdf_50207': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50207,\n",
       "  'change_nature': 'held in singapore technologies engineering lxx'},\n",
       " '301844.pdf_50208': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50208,\n",
       "  'change_nature': 'held in singapore technologies engineering lxx'},\n",
       " '301844.pdf_50211': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50211,\n",
       "  'change_nature': 'held in smr corporation lxx'},\n",
       " '301844.pdf_50212': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50212,\n",
       "  'change_nature': 'held in singapore telecommunications lxx'},\n",
       " '301844.pdf_50213': {'column_to_test': 'change_nature',\n",
       "  'filename': '301844.pdf',\n",
       "  'index': 50213,\n",
       "  'change_nature': 'held in stats chippac lxx'},\n",
       " '445722.pdf_58005': {'column_to_test': 'change_nature',\n",
       "  'filename': '445722.pdf',\n",
       "  'index': 58005,\n",
       "  'change_nature': 'xxxxx'},\n",
       " '773331.pdf_67824': {'column_to_test': 'change_nature',\n",
       "  'filename': '773331.pdf',\n",
       "  'index': 67824,\n",
       "  'change_nature': 'xxxx No cessation date'},\n",
       " '773331.pdf_67825': {'column_to_test': 'change_nature',\n",
       "  'filename': '773331.pdf',\n",
       "  'index': 67825,\n",
       "  'change_nature': 'xxxx No cessation date'},\n",
       " '773331.pdf_67826': {'column_to_test': 'change_nature',\n",
       "  'filename': '773331.pdf',\n",
       "  'index': 67826,\n",
       "  'change_nature': 'xxxx No cessation date'},\n",
       " '773331.pdf_67827': {'column_to_test': 'change_nature',\n",
       "  'filename': '773331.pdf',\n",
       "  'index': 67827,\n",
       "  'change_nature': 'xxxx No cessation date'},\n",
       " '773331.pdf_67828': {'column_to_test': 'change_nature',\n",
       "  'filename': '773331.pdf',\n",
       "  'index': 67828,\n",
       "  'change_nature': 'xxxx No cessation date'}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_unwanted_x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_x_key_list = []\n",
    "\n",
    "for k, v in check_for_unwanted_x_dict.items():\n",
    "    \n",
    "    unwanted_x_key_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-6ce917fa3188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0melex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwanted_x_key_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfilename_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_for_unwanted_x_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindex_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_for_unwanted_x_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchange_nature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_for_unwanted_x_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'change_nature'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "elex = unwanted_x_key_list[16]\n",
    "\n",
    "filename_ = check_for_unwanted_x_dict[elex]['filename']\n",
    "index_ = check_for_unwanted_x_dict[elex]['index']\n",
    "change_nature = check_for_unwanted_x_dict[elex]['change_nature']\n",
    "dir_name = df_check_dates.loc[index_, 'director_name_round_four']\n",
    "\n",
    "print(filename_, '|', index_, '|', change_nature, '|', df_check_dates.loc[index_, 'change_nature'], '|', dir_name)\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "    &\n",
    "    (df_check_dates['director_name_round_four'] == dir_name)\n",
    "]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.at[index_, 'change_nature'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>rec_id</th>\n",
       "      <th>file_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>rec_type0</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>received_datetime</th>\n",
       "      <th>released_datetime</th>\n",
       "      <th>number_of_entities</th>\n",
       "      <th>exchange_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>director_name</th>\n",
       "      <th>last_notice_date</th>\n",
       "      <th>cover_sheet_date</th>\n",
       "      <th>holder_name</th>\n",
       "      <th>interest_type</th>\n",
       "      <th>appointment_date</th>\n",
       "      <th>change_date</th>\n",
       "      <th>cessation_date</th>\n",
       "      <th>event_date</th>\n",
       "      <th>security_name_long</th>\n",
       "      <th>security_name_short</th>\n",
       "      <th>number</th>\n",
       "      <th>prior_number</th>\n",
       "      <th>acquired_number</th>\n",
       "      <th>disposed_number</th>\n",
       "      <th>after_number</th>\n",
       "      <th>change_nature</th>\n",
       "      <th>consideration</th>\n",
       "      <th>consideration_per_security</th>\n",
       "      <th>exercise_price</th>\n",
       "      <th>expiry_date</th>\n",
       "      <th>load_id</th>\n",
       "      <th>date_added</th>\n",
       "      <th>datetime</th>\n",
       "      <th>status</th>\n",
       "      <th>description</th>\n",
       "      <th>_merge_3X3Y3Z</th>\n",
       "      <th>_merge_load_log</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>director_name_string</th>\n",
       "      <th>director_name_changed</th>\n",
       "      <th>director_name_changed_bool</th>\n",
       "      <th>ticker_reviewed_bool</th>\n",
       "      <th>entity_name_changed</th>\n",
       "      <th>entity_name_changed_bool</th>\n",
       "      <th>entity_name_reviewed_bool</th>\n",
       "      <th>director_name_final</th>\n",
       "      <th>director_name_final_bool</th>\n",
       "      <th>director_name_overall</th>\n",
       "      <th>director_name_overall_bool</th>\n",
       "      <th>alternative_name</th>\n",
       "      <th>director_name_round_two</th>\n",
       "      <th>director_name_round_two_bool</th>\n",
       "      <th>director_name_round_one</th>\n",
       "      <th>director_name_round_one_bool</th>\n",
       "      <th>entity_name_corrected</th>\n",
       "      <th>entity_name_final</th>\n",
       "      <th>entity_name_final_ticker_checked</th>\n",
       "      <th>entity_name_final_bool</th>\n",
       "      <th>orig_index_I</th>\n",
       "      <th>ASX_change_bool</th>\n",
       "      <th>ticker_changes</th>\n",
       "      <th>name_changes</th>\n",
       "      <th>last_ticker</th>\n",
       "      <th>last_name</th>\n",
       "      <th>ASX_change_dates</th>\n",
       "      <th>last_ASX_change_date</th>\n",
       "      <th>ticker_to_display</th>\n",
       "      <th>name_to_display</th>\n",
       "      <th>delisted_bool</th>\n",
       "      <th>delisted_date</th>\n",
       "      <th>orig_index_II</th>\n",
       "      <th>director_name_round_three</th>\n",
       "      <th>director_name_round_three_bool</th>\n",
       "      <th>director_name_round_four_old</th>\n",
       "      <th>director_name_round_four_bool</th>\n",
       "      <th>corrected_names_14Nov19_bool</th>\n",
       "      <th>director_name_round_four</th>\n",
       "      <th>director_name_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38197</th>\n",
       "      <td>773334.pdf</td>\n",
       "      <td>329268</td>\n",
       "      <td>33632.0</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:17:09</td>\n",
       "      <td>2013-12-11 14:17:09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>S.C. Ottrey</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Sarah Christine Ottrey &amp; Peter Calder Groves</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 07:38:12</td>\n",
       "      <td>2019-07-05 07:17:29</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>773334.pdf32926833632.03Z</td>\n",
       "      <td>S.C Ottrey</td>\n",
       "      <td>S C Ottrey</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S C Ottrey</td>\n",
       "      <td>0</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>0</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19485</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19485</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>0</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sarah Ottrey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50671</th>\n",
       "      <td>773330.pdf</td>\n",
       "      <td>448961</td>\n",
       "      <td>51303.0</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:08:17</td>\n",
       "      <td>2013-12-11 14:08:17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>E.M. Coutts</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Como Nominees Limited</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>26066.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:32:19</td>\n",
       "      <td>2019-07-05 07:21:16</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>773330.pdf44896151303.03Z</td>\n",
       "      <td>E.M Coutts</td>\n",
       "      <td>E M Coutts</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E M Coutts</td>\n",
       "      <td>0</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>0</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>235898</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235884</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>0</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elizabeth Coutts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63213</th>\n",
       "      <td>773328.pdf</td>\n",
       "      <td>449512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:06:18</td>\n",
       "      <td>2013-12-11 14:06:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>B.J. Wallace</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Whyte Adder No 3 Limited</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>6879236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:33:04</td>\n",
       "      <td>2019-07-05 07:18:04</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773328.pdf449512nan3Z</td>\n",
       "      <td>B.J Wallace</td>\n",
       "      <td>B J Wallace</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>B J Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brett Wallace</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236899</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236885</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63214</th>\n",
       "      <td>773328.pdf</td>\n",
       "      <td>449512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:06:18</td>\n",
       "      <td>2013-12-11 14:06:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>B.J. Wallace</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Herpa Properties Limited</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>1302960.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:33:04</td>\n",
       "      <td>2019-07-05 07:18:04</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773328.pdf449512nan3Z</td>\n",
       "      <td>B.J Wallace</td>\n",
       "      <td>B J Wallace</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>B J Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brett Wallace</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236899</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236885</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Brent Wallace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64286</th>\n",
       "      <td>773332.pdf</td>\n",
       "      <td>447003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>P.F. Kraus</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>Direct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:29:30</td>\n",
       "      <td>2019-07-05 07:20:20</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773332.pdf447003nan3Z</td>\n",
       "      <td>P.F Kraus</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232436</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232422</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64287</th>\n",
       "      <td>773332.pdf</td>\n",
       "      <td>447003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>P.F. Kraus</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Whyte Adder No 3 Limited</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>6879236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:29:30</td>\n",
       "      <td>2019-07-05 07:20:20</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773332.pdf447003nan3Z</td>\n",
       "      <td>P.F Kraus</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232436</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232422</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64288</th>\n",
       "      <td>773332.pdf</td>\n",
       "      <td>447003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>2013-12-11 14:13:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>P.F. Kraus</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Herpa Properties Limited</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>1302960.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:29:30</td>\n",
       "      <td>2019-07-05 07:20:20</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773332.pdf447003nan3Z</td>\n",
       "      <td>P.F Kraus</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P F Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232436</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232422</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peter Kraus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67824</th>\n",
       "      <td>773331.pdf</td>\n",
       "      <td>360614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>M.B. Waller</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Mark Waller &amp; Angela Laura Waller</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>519986.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxxx No cessation date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 08:23:16</td>\n",
       "      <td>2019-07-05 07:16:43</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773331.pdf360614nan3Z</td>\n",
       "      <td>M.B Waller</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76729</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76725</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67825</th>\n",
       "      <td>773331.pdf</td>\n",
       "      <td>360614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>M.B. Waller</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Marie June Waller</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>4115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxxx No cessation date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 08:23:16</td>\n",
       "      <td>2019-07-05 07:16:43</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773331.pdf360614nan3Z</td>\n",
       "      <td>M.B Waller</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76729</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76725</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67826</th>\n",
       "      <td>773331.pdf</td>\n",
       "      <td>360614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>M.B. Waller</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Scott James Waller</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxxx No cessation date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 08:23:16</td>\n",
       "      <td>2019-07-05 07:16:43</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773331.pdf360614nan3Z</td>\n",
       "      <td>M.B Waller</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76729</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76725</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67827</th>\n",
       "      <td>773331.pdf</td>\n",
       "      <td>360614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>M.B. Waller</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Estate NJ Waller</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxxx No cessation date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 08:23:16</td>\n",
       "      <td>2019-07-05 07:16:43</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773331.pdf360614nan3Z</td>\n",
       "      <td>M.B Waller</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76729</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76725</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67828</th>\n",
       "      <td>773331.pdf</td>\n",
       "      <td>360614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-07</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>2013-12-11 14:11:03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>M.B. Waller</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>RGM Christie &amp; Mark Waller ATF the EBOS Staff ...</td>\n",
       "      <td>Indirect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>2013-11-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>145642.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xxxx No cessation date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 08:23:16</td>\n",
       "      <td>2019-07-05 07:16:43</td>\n",
       "      <td>document-only</td>\n",
       "      <td>Exception in Processiong 3Z Appendix Part-2</td>\n",
       "      <td>left_only</td>\n",
       "      <td>both</td>\n",
       "      <td>773331.pdf360614nan3Z</td>\n",
       "      <td>M.B Waller</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M B Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>10</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76729</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EBO</td>\n",
       "      <td>EBOS GROUP LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76725</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mark Waller</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  pdf_id   rec_id  file_date document_type  rec_type0  \\\n",
       "38197  773334.pdf  329268  33632.0 2019-02-07            3Z       2010   \n",
       "50671  773330.pdf  448961  51303.0 2019-02-07            3Z       2010   \n",
       "63213  773328.pdf  449512      NaN 2019-02-07            3Z       2010   \n",
       "63214  773328.pdf  449512      NaN 2019-02-07            3Z       2010   \n",
       "64286  773332.pdf  447003      NaN 2019-02-07            3Z       2010   \n",
       "64287  773332.pdf  447003      NaN 2019-02-07            3Z       2010   \n",
       "64288  773332.pdf  447003      NaN 2019-02-07            3Z       2010   \n",
       "67824  773331.pdf  360614      NaN 2019-02-07            3Z       2010   \n",
       "67825  773331.pdf  360614      NaN 2019-02-07            3Z       2010   \n",
       "67826  773331.pdf  360614      NaN 2019-02-07            3Z       2010   \n",
       "67827  773331.pdf  360614      NaN 2019-02-07            3Z       2010   \n",
       "67828  773331.pdf  360614      NaN 2019-02-07            3Z       2010   \n",
       "\n",
       "       rec_type   received_datetime   released_datetime  number_of_entities  \\\n",
       "38197       NaN 2013-12-11 14:17:09 2013-12-11 14:17:09                   1   \n",
       "50671       NaN 2013-12-11 14:08:17 2013-12-11 14:08:17                   1   \n",
       "63213       NaN 2013-12-11 14:06:18 2013-12-11 14:06:18                   1   \n",
       "63214       NaN 2013-12-11 14:06:18 2013-12-11 14:06:18                   1   \n",
       "64286       NaN 2013-12-11 14:13:27 2013-12-11 14:13:27                   1   \n",
       "64287       NaN 2013-12-11 14:13:27 2013-12-11 14:13:27                   1   \n",
       "64288       NaN 2013-12-11 14:13:27 2013-12-11 14:13:27                   1   \n",
       "67824       NaN 2013-12-11 14:11:03 2013-12-11 14:11:03                   1   \n",
       "67825       NaN 2013-12-11 14:11:03 2013-12-11 14:11:03                   1   \n",
       "67826       NaN 2013-12-11 14:11:03 2013-12-11 14:11:03                   1   \n",
       "67827       NaN 2013-12-11 14:11:03 2013-12-11 14:11:03                   1   \n",
       "67828       NaN 2013-12-11 14:11:03 2013-12-11 14:11:03                   1   \n",
       "\n",
       "       exchange_id ticker         entity_name director_name last_notice_date  \\\n",
       "38197            0    EBO  EBOS GROUP LIMITED   S.C. Ottrey       2013-11-07   \n",
       "50671            0    EBO  EBOS GROUP LIMITED   E.M. Coutts       2013-11-07   \n",
       "63213            0    EBO  EBOS GROUP LIMITED  B.J. Wallace       2013-11-07   \n",
       "63214            0    EBO  EBOS GROUP LIMITED  B.J. Wallace       2013-11-07   \n",
       "64286            0    EBO  EBOS GROUP LIMITED    P.F. Kraus       2013-11-07   \n",
       "64287            0    EBO  EBOS GROUP LIMITED    P.F. Kraus       2013-11-07   \n",
       "64288            0    EBO  EBOS GROUP LIMITED    P.F. Kraus       2013-11-07   \n",
       "67824            0    EBO  EBOS GROUP LIMITED   M.B. Waller       2013-11-07   \n",
       "67825            0    EBO  EBOS GROUP LIMITED   M.B. Waller       2013-11-07   \n",
       "67826            0    EBO  EBOS GROUP LIMITED   M.B. Waller       2013-11-07   \n",
       "67827            0    EBO  EBOS GROUP LIMITED   M.B. Waller       2013-11-07   \n",
       "67828            0    EBO  EBOS GROUP LIMITED   M.B. Waller       2013-11-07   \n",
       "\n",
       "      cover_sheet_date                                        holder_name  \\\n",
       "38197              NaT       Sarah Christine Ottrey & Peter Calder Groves   \n",
       "50671              NaT                              Como Nominees Limited   \n",
       "63213              NaT                           Whyte Adder No 3 Limited   \n",
       "63214              NaT                           Herpa Properties Limited   \n",
       "64286              NaT                                        Peter Kraus   \n",
       "64287              NaT                           Whyte Adder No 3 Limited   \n",
       "64288              NaT                           Herpa Properties Limited   \n",
       "67824              NaT                  Mark Waller & Angela Laura Waller   \n",
       "67825              NaT                                  Marie June Waller   \n",
       "67826              NaT                                 Scott James Waller   \n",
       "67827              NaT                                   Estate NJ Waller   \n",
       "67828              NaT  RGM Christie & Mark Waller ATF the EBOS Staff ...   \n",
       "\n",
       "      interest_type appointment_date change_date cessation_date  event_date  \\\n",
       "38197      Indirect              NaN         NaN     2013-12-11  2013-12-11   \n",
       "50671      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "63213      Indirect              NaN         NaN     2013-12-11  2013-12-11   \n",
       "63214      Indirect              NaN         NaN     2013-12-11  2013-12-11   \n",
       "64286        Direct              NaN         NaN     2013-12-11  2013-12-11   \n",
       "64287      Indirect              NaN         NaN     2013-12-11  2013-12-11   \n",
       "64288      Indirect              NaN         NaN     2013-12-11  2013-12-11   \n",
       "67824      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "67825      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "67826      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "67827      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "67828      Indirect              NaN         NaN     2013-11-07  2013-11-07   \n",
       "\n",
       "      security_name_long security_name_short     number prior_number  \\\n",
       "38197    Ordinary Shares              Shares     7580.0            0   \n",
       "50671    Ordinary Shares              Shares    26066.0            0   \n",
       "63213    Ordinary Shares              Shares  6879236.0          NaN   \n",
       "63214    Ordinary Shares              Shares  1302960.0          NaN   \n",
       "64286    Ordinary Shares              Shares     1535.0          NaN   \n",
       "64287    Ordinary Shares              Shares  6879236.0          NaN   \n",
       "64288    Ordinary Shares              Shares  1302960.0          NaN   \n",
       "67824    Ordinary Shares              Shares   519986.0          NaN   \n",
       "67825    Ordinary Shares              Shares     4115.0          NaN   \n",
       "67826    Ordinary Shares              Shares      880.0          NaN   \n",
       "67827    Ordinary Shares              Shares     5196.0          NaN   \n",
       "67828    Ordinary Shares              Shares   145642.0          NaN   \n",
       "\n",
       "      acquired_number  disposed_number  after_number           change_nature  \\\n",
       "38197               0              0.0           0.0                       0   \n",
       "50671               0              0.0           0.0                       0   \n",
       "63213             NaN              NaN           NaN                     NaN   \n",
       "63214             NaN              NaN           NaN                     NaN   \n",
       "64286             NaN              NaN           NaN                     NaN   \n",
       "64287             NaN              NaN           NaN                     NaN   \n",
       "64288             NaN              NaN           NaN                     NaN   \n",
       "67824             NaN              NaN           NaN  xxxx No cessation date   \n",
       "67825             NaN              NaN           NaN  xxxx No cessation date   \n",
       "67826             NaN              NaN           NaN  xxxx No cessation date   \n",
       "67827             NaN              NaN           NaN  xxxx No cessation date   \n",
       "67828             NaN              NaN           NaN  xxxx No cessation date   \n",
       "\n",
       "       consideration  consideration_per_security  exercise_price expiry_date  \\\n",
       "38197            0.0                         0.0             NaN         NaN   \n",
       "50671            0.0                         0.0             NaN         NaN   \n",
       "63213            NaN                         NaN             NaN         NaN   \n",
       "63214            NaN                         NaN             NaN         NaN   \n",
       "64286            NaN                         NaN             NaN         NaN   \n",
       "64287            NaN                         NaN             NaN         NaN   \n",
       "64288            NaN                         NaN             NaN         NaN   \n",
       "67824            NaN                         NaN             NaN         NaN   \n",
       "67825            NaN                         NaN             NaN         NaN   \n",
       "67826            NaN                         NaN             NaN         NaN   \n",
       "67827            NaN                         NaN             NaN         NaN   \n",
       "67828            NaN                         NaN             NaN         NaN   \n",
       "\n",
       "                                    load_id          date_added  \\\n",
       "38197  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 07:38:12   \n",
       "50671  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:32:19   \n",
       "63213  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:33:04   \n",
       "63214  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:33:04   \n",
       "64286  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:29:30   \n",
       "64287  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:29:30   \n",
       "64288  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:29:30   \n",
       "67824  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 08:23:16   \n",
       "67825  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 08:23:16   \n",
       "67826  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 08:23:16   \n",
       "67827  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 08:23:16   \n",
       "67828  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 08:23:16   \n",
       "\n",
       "                 datetime         status  \\\n",
       "38197 2019-07-05 07:17:29        success   \n",
       "50671 2019-07-05 07:21:16        success   \n",
       "63213 2019-07-05 07:18:04  document-only   \n",
       "63214 2019-07-05 07:18:04  document-only   \n",
       "64286 2019-07-05 07:20:20  document-only   \n",
       "64287 2019-07-05 07:20:20  document-only   \n",
       "64288 2019-07-05 07:20:20  document-only   \n",
       "67824 2019-07-05 07:16:43  document-only   \n",
       "67825 2019-07-05 07:16:43  document-only   \n",
       "67826 2019-07-05 07:16:43  document-only   \n",
       "67827 2019-07-05 07:16:43  document-only   \n",
       "67828 2019-07-05 07:16:43  document-only   \n",
       "\n",
       "                                       description _merge_3X3Y3Z  \\\n",
       "38197                                          NaN          both   \n",
       "50671                                          NaN          both   \n",
       "63213  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "63214  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "64286  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "64287  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "64288  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "67824  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "67825  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "67826  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "67827  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "67828  Exception in Processiong 3Z Appendix Part-2     left_only   \n",
       "\n",
       "      _merge_load_log                  unique_id director_name_string  \\\n",
       "38197            both  773334.pdf32926833632.03Z           S.C Ottrey   \n",
       "50671            both  773330.pdf44896151303.03Z           E.M Coutts   \n",
       "63213            both      773328.pdf449512nan3Z          B.J Wallace   \n",
       "63214            both      773328.pdf449512nan3Z          B.J Wallace   \n",
       "64286            both      773332.pdf447003nan3Z            P.F Kraus   \n",
       "64287            both      773332.pdf447003nan3Z            P.F Kraus   \n",
       "64288            both      773332.pdf447003nan3Z            P.F Kraus   \n",
       "67824            both      773331.pdf360614nan3Z           M.B Waller   \n",
       "67825            both      773331.pdf360614nan3Z           M.B Waller   \n",
       "67826            both      773331.pdf360614nan3Z           M.B Waller   \n",
       "67827            both      773331.pdf360614nan3Z           M.B Waller   \n",
       "67828            both      773331.pdf360614nan3Z           M.B Waller   \n",
       "\n",
       "      director_name_changed  director_name_changed_bool  ticker_reviewed_bool  \\\n",
       "38197            S C Ottrey                           1                     1   \n",
       "50671            E M Coutts                           1                     1   \n",
       "63213           B J Wallace                           1                     1   \n",
       "63214           B J Wallace                           1                     1   \n",
       "64286             P F Kraus                           1                     1   \n",
       "64287             P F Kraus                           1                     1   \n",
       "64288             P F Kraus                           1                     1   \n",
       "67824            M B Waller                           1                     1   \n",
       "67825            M B Waller                           1                     1   \n",
       "67826            M B Waller                           1                     1   \n",
       "67827            M B Waller                           1                     1   \n",
       "67828            M B Waller                           1                     1   \n",
       "\n",
       "      entity_name_changed  entity_name_changed_bool  \\\n",
       "38197  EBOS GROUP LIMITED                         0   \n",
       "50671  EBOS GROUP LIMITED                         0   \n",
       "63213  EBOS GROUP LIMITED                         0   \n",
       "63214  EBOS GROUP LIMITED                         0   \n",
       "64286  EBOS GROUP LIMITED                         0   \n",
       "64287  EBOS GROUP LIMITED                         0   \n",
       "64288  EBOS GROUP LIMITED                         0   \n",
       "67824  EBOS GROUP LIMITED                         0   \n",
       "67825  EBOS GROUP LIMITED                         0   \n",
       "67826  EBOS GROUP LIMITED                         0   \n",
       "67827  EBOS GROUP LIMITED                         0   \n",
       "67828  EBOS GROUP LIMITED                         0   \n",
       "\n",
       "       entity_name_reviewed_bool director_name_final  \\\n",
       "38197                          0          S C Ottrey   \n",
       "50671                          0          E M Coutts   \n",
       "63213                          0         B J Wallace   \n",
       "63214                          0         B J Wallace   \n",
       "64286                          0           P F Kraus   \n",
       "64287                          0           P F Kraus   \n",
       "64288                          0           P F Kraus   \n",
       "67824                          0          M B Waller   \n",
       "67825                          0          M B Waller   \n",
       "67826                          0          M B Waller   \n",
       "67827                          0          M B Waller   \n",
       "67828                          0          M B Waller   \n",
       "\n",
       "       director_name_final_bool director_name_overall  \\\n",
       "38197                         0          Sarah Ottrey   \n",
       "50671                         0      Elizabeth Coutts   \n",
       "63213                         0         Brett Wallace   \n",
       "63214                         0         Brett Wallace   \n",
       "64286                         0           Peter Kraus   \n",
       "64287                         0           Peter Kraus   \n",
       "64288                         0           Peter Kraus   \n",
       "67824                         0           Mark Waller   \n",
       "67825                         0           Mark Waller   \n",
       "67826                         0           Mark Waller   \n",
       "67827                         0           Mark Waller   \n",
       "67828                         0           Mark Waller   \n",
       "\n",
       "       director_name_overall_bool alternative_name director_name_round_two  \\\n",
       "38197                          10              NaN            Sarah Ottrey   \n",
       "50671                          10              NaN        Elizabeth Coutts   \n",
       "63213                          10              NaN           Brent Wallace   \n",
       "63214                          10              NaN           Brent Wallace   \n",
       "64286                          10              NaN             Peter Kraus   \n",
       "64287                          10              NaN             Peter Kraus   \n",
       "64288                          10              NaN             Peter Kraus   \n",
       "67824                          10              NaN             Mark Waller   \n",
       "67825                          10              NaN             Mark Waller   \n",
       "67826                          10              NaN             Mark Waller   \n",
       "67827                          10              NaN             Mark Waller   \n",
       "67828                          10              NaN             Mark Waller   \n",
       "\n",
       "       director_name_round_two_bool director_name_round_one  \\\n",
       "38197                             0            Sarah Ottrey   \n",
       "50671                             0        Elizabeth Coutts   \n",
       "63213                             0           Brent Wallace   \n",
       "63214                             0           Brent Wallace   \n",
       "64286                             0             Peter Kraus   \n",
       "64287                             0             Peter Kraus   \n",
       "64288                             0             Peter Kraus   \n",
       "67824                             0             Mark Waller   \n",
       "67825                             0             Mark Waller   \n",
       "67826                             0             Mark Waller   \n",
       "67827                             0             Mark Waller   \n",
       "67828                             0             Mark Waller   \n",
       "\n",
       "       director_name_round_one_bool entity_name_corrected   entity_name_final  \\\n",
       "38197                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "50671                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "63213                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "63214                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "64286                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "64287                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "64288                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "67824                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "67825                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "67826                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "67827                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "67828                            10    EBOS GROUP LIMITED  EBOS GROUP LIMITED   \n",
       "\n",
       "       entity_name_final_ticker_checked  entity_name_final_bool  orig_index_I  \\\n",
       "38197                                 0                       0         19485   \n",
       "50671                                 0                       0        235898   \n",
       "63213                                 0                       0        236899   \n",
       "63214                                 0                       0        236899   \n",
       "64286                                 0                       0        232436   \n",
       "64287                                 0                       0        232436   \n",
       "64288                                 0                       0        232436   \n",
       "67824                                 0                       0         76729   \n",
       "67825                                 0                       0         76729   \n",
       "67826                                 0                       0         76729   \n",
       "67827                                 0                       0         76729   \n",
       "67828                                 0                       0         76729   \n",
       "\n",
       "       ASX_change_bool ticker_changes name_changes last_ticker last_name  \\\n",
       "38197                0            NaN          NaN         NaN       NaN   \n",
       "50671                0            NaN          NaN         NaN       NaN   \n",
       "63213                0            NaN          NaN         NaN       NaN   \n",
       "63214                0            NaN          NaN         NaN       NaN   \n",
       "64286                0            NaN          NaN         NaN       NaN   \n",
       "64287                0            NaN          NaN         NaN       NaN   \n",
       "64288                0            NaN          NaN         NaN       NaN   \n",
       "67824                0            NaN          NaN         NaN       NaN   \n",
       "67825                0            NaN          NaN         NaN       NaN   \n",
       "67826                0            NaN          NaN         NaN       NaN   \n",
       "67827                0            NaN          NaN         NaN       NaN   \n",
       "67828                0            NaN          NaN         NaN       NaN   \n",
       "\n",
       "      ASX_change_dates last_ASX_change_date ticker_to_display  \\\n",
       "38197              NaT                  NaT               EBO   \n",
       "50671              NaT                  NaT               EBO   \n",
       "63213              NaT                  NaT               EBO   \n",
       "63214              NaT                  NaT               EBO   \n",
       "64286              NaT                  NaT               EBO   \n",
       "64287              NaT                  NaT               EBO   \n",
       "64288              NaT                  NaT               EBO   \n",
       "67824              NaT                  NaT               EBO   \n",
       "67825              NaT                  NaT               EBO   \n",
       "67826              NaT                  NaT               EBO   \n",
       "67827              NaT                  NaT               EBO   \n",
       "67828              NaT                  NaT               EBO   \n",
       "\n",
       "          name_to_display  delisted_bool delisted_date  orig_index_II  \\\n",
       "38197  EBOS GROUP LIMITED              0           NaN          19485   \n",
       "50671  EBOS GROUP LIMITED              0           NaN         235884   \n",
       "63213  EBOS GROUP LIMITED              0           NaN         236885   \n",
       "63214  EBOS GROUP LIMITED              0           NaN         236885   \n",
       "64286  EBOS GROUP LIMITED              0           NaN         232422   \n",
       "64287  EBOS GROUP LIMITED              0           NaN         232422   \n",
       "64288  EBOS GROUP LIMITED              0           NaN         232422   \n",
       "67824  EBOS GROUP LIMITED              0           NaN          76725   \n",
       "67825  EBOS GROUP LIMITED              0           NaN          76725   \n",
       "67826  EBOS GROUP LIMITED              0           NaN          76725   \n",
       "67827  EBOS GROUP LIMITED              0           NaN          76725   \n",
       "67828  EBOS GROUP LIMITED              0           NaN          76725   \n",
       "\n",
       "      director_name_round_three  director_name_round_three_bool  \\\n",
       "38197              Sarah Ottrey                               0   \n",
       "50671          Elizabeth Coutts                               0   \n",
       "63213             Brent Wallace                               0   \n",
       "63214             Brent Wallace                               0   \n",
       "64286               Peter Kraus                               0   \n",
       "64287               Peter Kraus                               0   \n",
       "64288               Peter Kraus                               0   \n",
       "67824               Mark Waller                               0   \n",
       "67825               Mark Waller                               0   \n",
       "67826               Mark Waller                               0   \n",
       "67827               Mark Waller                               0   \n",
       "67828               Mark Waller                               0   \n",
       "\n",
       "      director_name_round_four_old  director_name_round_four_bool  \\\n",
       "38197                 Sarah Ottrey                              0   \n",
       "50671             Elizabeth Coutts                              0   \n",
       "63213                Brent Wallace                              0   \n",
       "63214                Brent Wallace                              0   \n",
       "64286                  Peter Kraus                              0   \n",
       "64287                  Peter Kraus                              0   \n",
       "64288                  Peter Kraus                              0   \n",
       "67824                  Mark Waller                              0   \n",
       "67825                  Mark Waller                              0   \n",
       "67826                  Mark Waller                              0   \n",
       "67827                  Mark Waller                              0   \n",
       "67828                  Mark Waller                              0   \n",
       "\n",
       "       corrected_names_14Nov19_bool director_name_round_four  \\\n",
       "38197                             0             Sarah Ottrey   \n",
       "50671                             0         Elizabeth Coutts   \n",
       "63213                             0            Brent Wallace   \n",
       "63214                             0            Brent Wallace   \n",
       "64286                             0              Peter Kraus   \n",
       "64287                             0              Peter Kraus   \n",
       "64288                             0              Peter Kraus   \n",
       "67824                             0              Mark Waller   \n",
       "67825                             0              Mark Waller   \n",
       "67826                             0              Mark Waller   \n",
       "67827                             0              Mark Waller   \n",
       "67828                             0              Mark Waller   \n",
       "\n",
       "       director_name_corrected  \n",
       "38197                        0  \n",
       "50671                        0  \n",
       "63213                        0  \n",
       "63214                        0  \n",
       "64286                        0  \n",
       "64287                        0  \n",
       "64288                        0  \n",
       "67824                        0  \n",
       "67825                        0  \n",
       "67826                        0  \n",
       "67827                        0  \n",
       "67828                        0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_dates[\n",
    "    (df_check_dates['name_to_display'] == 'EBOS GROUP LIMITED')\n",
    "#     &\n",
    "#     (df_check_dates['director_name_round_four'] == dir_name)\n",
    "    &\n",
    "    (df_check_dates['document_type'] == '3Z')\n",
    "    &\n",
    "    (df_check_dates['event_date'].str.contains('2013'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan B Trustees Limited ATF B & E Super Fund\n"
     ]
    }
   ],
   "source": [
    "regex_pattern_ampersand = re.compile(r\"&\", re.I)\n",
    "regex_pattern_multi_spaces = re.compile(r\"\\s{2,}\", re.I)\n",
    "\n",
    "test_str = \"Plan B Trustees Limited ATF B & E Super Fund\"\n",
    "\n",
    "corrected_string_I = regex_pattern_ampersand.sub(r' & ', test_str)\n",
    "corrected_string_II = regex_pattern_multi_spaces.sub(r' ', corrected_string_I)\n",
    "\n",
    "print(corrected_string_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern_super_fund = re.compile(r\"super\\s+fund\", re.I)\n",
    "regex_pattern_ampersand = re.compile(r\"&\", re.I)\n",
    "regex_pattern_multi_spaces = re.compile(r\"\\s{2,}\", re.I)\n",
    "\n",
    "for idx, elex in enumerate(unwanted_x_key_list):\n",
    "\n",
    "    filename_ = check_for_unwanted_x_dict[elex]['filename']\n",
    "    index_ = check_for_unwanted_x_dict[elex]['index']\n",
    "    holder_name_ = check_for_unwanted_x_dict[elex]['holder_name']\n",
    "    dir_name = df_check_dates.loc[index_, 'director_name_round_four']\n",
    "    \n",
    "    corrected_holder_name = regex_pattern_super_fund.sub('Superannuation Fund', holder_name_)\n",
    "    corrected_string_I = regex_pattern_ampersand.sub(r' & ', corrected_holder_name)\n",
    "    corrected_string_II = regex_pattern_multi_spaces.sub(r' ', corrected_string_I)\n",
    "    \n",
    "    if df_check_dates.loc[index_, 'holder_name'] == holder_name_ and df_check_dates.loc[index_, 'director_name_round_four'] == dir_name and df_check_dates.loc[index_, 'filename'] == filename_:\n",
    "        \n",
    "        df_check_dates.at[index_, 'holder_name'] = corrected_string_II\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('ERROR', k, v)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 E:\\announcements_COPIED\\2017\\06\\07\\01863734.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>pdf_id</th>\n",
       "      <th>rec_id</th>\n",
       "      <th>file_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>rec_type0</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>received_datetime</th>\n",
       "      <th>released_datetime</th>\n",
       "      <th>number_of_entities</th>\n",
       "      <th>exchange_id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>director_name</th>\n",
       "      <th>last_notice_date</th>\n",
       "      <th>cover_sheet_date</th>\n",
       "      <th>holder_name</th>\n",
       "      <th>interest_type</th>\n",
       "      <th>appointment_date</th>\n",
       "      <th>change_date</th>\n",
       "      <th>cessation_date</th>\n",
       "      <th>event_date</th>\n",
       "      <th>security_name_long</th>\n",
       "      <th>security_name_short</th>\n",
       "      <th>number</th>\n",
       "      <th>prior_number</th>\n",
       "      <th>acquired_number</th>\n",
       "      <th>disposed_number</th>\n",
       "      <th>after_number</th>\n",
       "      <th>change_nature</th>\n",
       "      <th>consideration</th>\n",
       "      <th>consideration_per_security</th>\n",
       "      <th>exercise_price</th>\n",
       "      <th>expiry_date</th>\n",
       "      <th>load_id</th>\n",
       "      <th>date_added</th>\n",
       "      <th>datetime</th>\n",
       "      <th>status</th>\n",
       "      <th>description</th>\n",
       "      <th>_merge_3X3Y3Z</th>\n",
       "      <th>_merge_load_log</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>director_name_string</th>\n",
       "      <th>director_name_changed</th>\n",
       "      <th>director_name_changed_bool</th>\n",
       "      <th>ticker_reviewed_bool</th>\n",
       "      <th>entity_name_changed</th>\n",
       "      <th>entity_name_changed_bool</th>\n",
       "      <th>entity_name_reviewed_bool</th>\n",
       "      <th>director_name_final</th>\n",
       "      <th>director_name_final_bool</th>\n",
       "      <th>director_name_overall</th>\n",
       "      <th>director_name_overall_bool</th>\n",
       "      <th>alternative_name</th>\n",
       "      <th>director_name_round_two</th>\n",
       "      <th>director_name_round_two_bool</th>\n",
       "      <th>director_name_round_one</th>\n",
       "      <th>director_name_round_one_bool</th>\n",
       "      <th>entity_name_corrected</th>\n",
       "      <th>entity_name_final</th>\n",
       "      <th>entity_name_final_ticker_checked</th>\n",
       "      <th>entity_name_final_bool</th>\n",
       "      <th>orig_index_I</th>\n",
       "      <th>ASX_change_bool</th>\n",
       "      <th>ticker_changes</th>\n",
       "      <th>name_changes</th>\n",
       "      <th>last_ticker</th>\n",
       "      <th>last_name</th>\n",
       "      <th>ASX_change_dates</th>\n",
       "      <th>last_ASX_change_date</th>\n",
       "      <th>ticker_to_display</th>\n",
       "      <th>name_to_display</th>\n",
       "      <th>delisted_bool</th>\n",
       "      <th>delisted_date</th>\n",
       "      <th>orig_index_II</th>\n",
       "      <th>director_name_round_three</th>\n",
       "      <th>director_name_round_three_bool</th>\n",
       "      <th>director_name_round_four_old</th>\n",
       "      <th>director_name_round_four_bool</th>\n",
       "      <th>corrected_names_14Nov19_bool</th>\n",
       "      <th>director_name_round_four</th>\n",
       "      <th>director_name_corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35968</th>\n",
       "      <td>01863734.pdf</td>\n",
       "      <td>427025</td>\n",
       "      <td>48071.0</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>3Z</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-07 10:22:01</td>\n",
       "      <td>2017-06-07 10:22:01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>MIH</td>\n",
       "      <td>MNC Media Investment Limited</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>Direct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>Ordinary Shares</td>\n",
       "      <td>Shares</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Date of last notice estimated as not clear fro...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fc315c06-e5ea-46d7-b2d1-43dfc6481789</td>\n",
       "      <td>2019-07-05 10:00:25</td>\n",
       "      <td>2019-07-05 07:18:36</td>\n",
       "      <td>success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "      <td>01863734.pdf42702548071.03Z</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MNC Media Investment Limited</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>MNC MEDIA INVESTMENT LIMITED</td>\n",
       "      <td>MNC MEDIA INVESTMENT LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196578</td>\n",
       "      <td>1</td>\n",
       "      <td>LTL, MIH</td>\n",
       "      <td>LINKETONE LIMITED, MNC MEDIA INVESTMENT LIMITED</td>\n",
       "      <td>MIH</td>\n",
       "      <td>MNC MEDIA INVESTMENT LIMITED</td>\n",
       "      <td>2014-07-10</td>\n",
       "      <td>2014-07-10</td>\n",
       "      <td>MIH</td>\n",
       "      <td>MNC MEDIA INVESTMENT LIMITED</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196568</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Peck Joo Tan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  pdf_id   rec_id  file_date document_type  rec_type0  \\\n",
       "35968  01863734.pdf  427025  48071.0 2019-02-05            3Z       2010   \n",
       "\n",
       "       rec_type   received_datetime   released_datetime  number_of_entities  \\\n",
       "35968       NaN 2017-06-07 10:22:01 2017-06-07 10:22:01                   1   \n",
       "\n",
       "       exchange_id ticker                   entity_name director_name  \\\n",
       "35968            3    MIH  MNC Media Investment Limited  Peck Joo Tan   \n",
       "\n",
       "      last_notice_date cover_sheet_date   holder_name interest_type  \\\n",
       "35968       2017-06-07              NaT  Peck Joo Tan        Direct   \n",
       "\n",
       "      appointment_date change_date cessation_date  event_date  \\\n",
       "35968              NaN         NaN     2017-06-07  2017-06-07   \n",
       "\n",
       "      security_name_long security_name_short    number prior_number  \\\n",
       "35968    Ordinary Shares              Shares  500000.0            0   \n",
       "\n",
       "      acquired_number  disposed_number  after_number  \\\n",
       "35968               0              0.0           0.0   \n",
       "\n",
       "                                           change_nature  consideration  \\\n",
       "35968  Date of last notice estimated as not clear fro...            0.0   \n",
       "\n",
       "       consideration_per_security  exercise_price expiry_date  \\\n",
       "35968                         0.0             NaN         NaN   \n",
       "\n",
       "                                    load_id          date_added  \\\n",
       "35968  fc315c06-e5ea-46d7-b2d1-43dfc6481789 2019-07-05 10:00:25   \n",
       "\n",
       "                 datetime   status description _merge_3X3Y3Z _merge_load_log  \\\n",
       "35968 2019-07-05 07:18:36  success         NaN          both            both   \n",
       "\n",
       "                         unique_id director_name_string director_name_changed  \\\n",
       "35968  01863734.pdf42702548071.03Z         Peck Joo Tan          Peck Joo Tan   \n",
       "\n",
       "       director_name_changed_bool  ticker_reviewed_bool  \\\n",
       "35968                           0                     1   \n",
       "\n",
       "                entity_name_changed  entity_name_changed_bool  \\\n",
       "35968  MNC Media Investment Limited                         0   \n",
       "\n",
       "       entity_name_reviewed_bool director_name_final  \\\n",
       "35968                          0        Peck Joo Tan   \n",
       "\n",
       "       director_name_final_bool director_name_overall  \\\n",
       "35968                         0          Peck Joo Tan   \n",
       "\n",
       "       director_name_overall_bool alternative_name director_name_round_two  \\\n",
       "35968                           0              NaN            Peck Joo Tan   \n",
       "\n",
       "       director_name_round_two_bool director_name_round_one  \\\n",
       "35968                             0            Peck Joo Tan   \n",
       "\n",
       "       director_name_round_one_bool         entity_name_corrected  \\\n",
       "35968                             0  MNC MEDIA INVESTMENT LIMITED   \n",
       "\n",
       "                  entity_name_final  entity_name_final_ticker_checked  \\\n",
       "35968  MNC MEDIA INVESTMENT LIMITED                                 0   \n",
       "\n",
       "       entity_name_final_bool  orig_index_I  ASX_change_bool ticker_changes  \\\n",
       "35968                       0        196578                1       LTL, MIH   \n",
       "\n",
       "                                          name_changes last_ticker  \\\n",
       "35968  LINKETONE LIMITED, MNC MEDIA INVESTMENT LIMITED         MIH   \n",
       "\n",
       "                          last_name ASX_change_dates last_ASX_change_date  \\\n",
       "35968  MNC MEDIA INVESTMENT LIMITED       2014-07-10           2014-07-10   \n",
       "\n",
       "      ticker_to_display               name_to_display  delisted_bool  \\\n",
       "35968               MIH  MNC MEDIA INVESTMENT LIMITED              0   \n",
       "\n",
       "      delisted_date  orig_index_II director_name_round_three  \\\n",
       "35968           NaN         196568              Peck Joo Tan   \n",
       "\n",
       "       director_name_round_three_bool director_name_round_four_old  \\\n",
       "35968                               0                 Peck Joo Tan   \n",
       "\n",
       "       director_name_round_four_bool  corrected_names_14Nov19_bool  \\\n",
       "35968                              0                             0   \n",
       "\n",
       "      director_name_round_four  director_name_corrected  \n",
       "35968             Peck Joo Tan                        0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key_ = unwanted_x_key_list[5]\n",
    "\n",
    "# filename_ = check_for_unwanted_x_dict[key_]['filename']\n",
    "\n",
    "filename_ = '01863734.pdf'\n",
    "# index_ = check_for_unwanted_x_dict[key_]['index']\n",
    "# holder_name_ = check_for_unwanted_x_dict[key_]['holder_name']\n",
    "# dir_name = df_check_dates.loc[index_, 'director_name_round_four']\n",
    "# dir_name = 'Scott Frew'\n",
    "# holder_name = 'Frew Nominees Pty Ltd'\n",
    "\n",
    "# print(filename_, '|', index_, '|', holder_name_, '|', df_check_dates.loc[index_, 'holder_name'], '|', dir_name)\n",
    "\n",
    "openPDFFileFromFileName(filename_)\n",
    "\n",
    "df_check_dates[\n",
    "    (df_check_dates['filename'] == filename_)\n",
    "#     &\n",
    "#     (df_check_dates['director_name_round_four'] == dir_name)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_dates.at[35968, 'change_nature'] = 'Date of last notice estimated as not clear from document'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
